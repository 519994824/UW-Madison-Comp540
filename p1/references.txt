LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
Yiran Ding * Li Lyna Zhang ‚Ä† Chengruidong Zhang Yuanyuan Xu * Ning Shang Jiahang Xu
Fan Yang Mao Yang
Microsoft Research

arXiv:2402.13753v1 [cs.CL] 21 Feb 2024

Abstract
Large context window is a desirable feature in
large language models (LLMs). However, due
to high fine-tuning costs, scarcity of long texts,
and catastrophic values introduced by new token
positions, current extended context windows are
limited to around 128k tokens.
This paper introduces LongRoPE that, for the first
time, extends the context window of pre-trained
LLMs to an impressive 2048k tokens, with up
to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the
original short context window. This is achieved
by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional
interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8√ó extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension
strategy that first fine-tunes a 256k length LLM
and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve
a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context
window performance. Extensive experiments on
LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models
extended via LongRoPE retain the original architecture with minor modifications to the positional
embedding, and can reuse most pre-existing optimizations. Code will be available at https:
//github.com/microsoft/LongRoPE

Figure 1. Books3 perplexity comparison between LongRoPE and
state-of-the-art long-context LLMs using other extension methods.

2023), often suffer from limited context window size, e.g.,
LLaMA2‚Äôs 4096 token limit (Touvron et al., 2023). Beyond
the context window, LLM‚Äôs performance declines due to the
additional positions that the model has not been trained on.
This poses challenges in important scenarios like in-context
learning with numerous examples (Huang et al., 2023) and
LLM agents (Park et al., 2023; Madaan et al., 2023).
Recent works show that a pre-trained LLM context window
can be extended to around 128k by fine-tuning on longer
texts (Chen et al., 2023b;a; Peng et al., 2023; Zhang et al.,
2024; Liu et al., 2023). There are three major obstacles
to further extend the context window. First, untrained new
position indices introduce many catastrophic values, leading
to out-of-distribution issues and making fine-tuning difficult to converge (Chen et al., 2023a). This is particularly
challenging when an extension from 4k to >1000k introduces more than 90% new positions. Second, fine-tuning
usually requires texts of corresponding lengths. However,
long texts in current datasets, especially those exceeding
1000k, are limited. Moreover, training on extra-long texts
is computationally expensive, requiring prohibitively extensive training hours and GPU resources. Third, when
extending to extremely long context windows, the attention
becomes dispersed as it‚Äôs spread thinly across numerous token positions, degrading performance on the original short
context (Chen et al., 2023a).

1. Introduction
Large Language Models (LLMs), despite remarkable success on various tasks (OpenAI et al., 2023; Touvron et al.,
*
Work was done during the internship. Yiran Ding: Hangzhou
Dianzi University; Yuanyuan Xu: University of Science and
Technology of China. Correspondence to: Li Lyna Zhang
<lzhani@microsoft.com>.

One approach to mitigate the first challenge is to interpolate
RoPE positional embedding (Su et al., 2021; Chen et al.,
2023a), which downscales new position indices to the pre-

1

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

trained range, as shown in Fig.2. Position Interpolation
(PI) (Chen et al., 2023a) linearly interpolates RoPE‚Äôs rotary angles by the extension ratio. NTK (LocalLLaMA,
2023b;a) advocates unequal interpolation and extrapolation
across RoPE dimensions. YaRN (Peng et al., 2023) categorizes RoPE dimensions into three frequency-based groups
and applies extrapolation, NTK, and linear interpolations,
respectively. However, positional embedding exhibits complex non-uniform information entropy in the Transformer
architecture. Such subtle non-uniformity is not effectively
leveraged by existing approaches, leading to information
loss and hence limiting the context window size.

search algorithm to encourage less positional interpolation.
During inference, if the sequence length is less than 8k, we
update RoPE with the searched rescale factors.
Extensive experiments across different LLMs and various
long-context tasks demonstrate the effectiveness of our
method. We show that LongRoPE is highly effective in
maintaining low perplexity from 4k to 2048k evaluation
length, achieving over 90% passkey retrieval accuracy, and
delivering comparable accuracy on standard benchmarks
designed within the 4096 context window. LongRoPE can
be applied to any LLMs based on RoPE embedding. We
will release our code and LongRoPE-2048k models.

Section 2 reveals two key findings empirically: (1) Effective
positional interpolation should consider two forms of nonuniformities: varying RoPE dimensions and token positions.
Lower RoPE dimensions and initial starting token positions
benefit from less interpolation, but the optimal solutions depend on the target extended length. (2) By considering these
non-uniformities into positional interpolation, we can effectively retain information in the original RoPE, particularly
key dimensions and token positions. This minimizes the
loss caused by positional interpolation, and thus provides
better initialization for fine-tuning. Moreover, it allows an
8√ó extension in non-fine-tuning scenarios.

2. Non-uniformity in Positional Interpolation
2.1. Preliminary
Transformer models require explicit positional information,
often in the form of position embedding, to represent the order of input tokens. Our work focuses on the RoPE (Su et al.,
2021) position embedding, which is widely used in recent
LLMs. For a token at position index n, its corresponding
RoPE encoding can be simplified as follows:
[cos(nŒ∏0 ), sin(nŒ∏0 ), cos(nŒ∏1 ), ¬∑ ¬∑ ¬∑, cos(nŒ∏d/2‚àí1 ), sin(nŒ∏d/2‚àí1 )]
(1)

where d is the embedding dimension, nŒ∏i is the rotary angle
of token at position n, Œ∏i = Œ∏‚àí2i/d represents the rotation
frequencies. In RoPE, the default base value of Œ∏ is 10000.

Motivated by the findings, we introduce LongRoPE, an
effective method that extends the LLM context window beyond 2 million tokens. LongRoPE is based on three key innovations. First, LongRoPE fully exploits multidimensional
non-uniformities in positional interpolation. It identifies
effective rescale factors for RoPE‚Äôs rotation angles for each
RoPE dimension, based on token positions. As the search
space that identifies rescale factors expands exponentially
with the target extension ratio, LongRoPE introduces an
evolutionary search algorithm with two optimization techniques to boost search efficiency. Fig. 2 shows an example
of the searched rescaled RoPE.

Context window extension ratio s and positional interpolation. We define s as the ratio of extended context length
‚Ä≤
L‚Ä≤ to the original length L: s = LL .
To extend the context window from L to L‚Ä≤ , current positional interpolation methods suggest downscaling rotation
frequencies Œ∏i based on extension ratio s. Let Œ≤ = Œ∏2/d ,
and Œª denote the actual rescale factor related to s, we unify
these positional interpolation methods as follows:

cos

Then, LongRoPE leverages an efficient, progressive extension strategy to achieve a 2048k context window without
the need of direct fine-tuning on texts with extremely long
lengths, which are scarce and hardly available. The strategy
begins by searching for a 256k length on the pre-trained
LLM and fine-tuning it under this length. Then, as our nonuniform positional interpolation allows for an 8√ó extension
in non-fine-tuning settings, we conduct a second search
for new RoPE rescale factors on the fine-tuned extended
LLM. This ultimately achieves the 2048k context window
for LLaMA2 and Mistral (Jiang et al., 2023).



n
Œª(Œ≤)0




, sin

n
Œª(Œ≤)0




, cos

n
Œª(Œ≤)1




, ¬∑ ¬∑ ¬∑, sin

n
Œª(Œ≤)d/2‚àí1



(2)

Linear positional interpolation (PI). PI (Chen et al.,
2023a) suggests linear interpolation of position indices
within the pre-trained length limit. For a target extension
ratio s, the rotation angles of all positions are linearly reduced by Œª = s across all RoPE dimensions. However,
this makes the position information very ‚Äúcrowded‚Äù, hindering the model‚Äôs ability distinguish closely positioned tokens.
Therefore, PI tends to underperform at high extension ratios.
NTK-based interpolation and extrapolation. (LocalLLaMA, 2023b;a) look at RoPE from an information encoding perspective and apply the Neural Tangent Kernel (NTK)
theory (Jacot et al., 2018; Tancik et al., 2020). To mitigate
the crowded-positions issue in PI, they suggest to distribute
interpolation pressure across RoPE dimensions. It scales

Finally, to mitigate performance degradation on the original
(shorter) context window, LongRoPE continues to adjust
the RoPE rescale factors on the extended LLM. Similar to
scaling up from 256k to 2048k, we scale down to 4k and
8k context windows on the 256k fine-tuned LLM using our
2

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

Unseen range

Pre-trained range

Extrapolation

Normal

Position Interpolation

No interpolation for initial ùëõ‡∑ú positions

LongRoPE: Non-uniform Position Interpolation and Extrapolation

position

Figure 2. An illustrative example to show RoPE embedding under different interpolation methods. Upper: RoPE under direct extrapolation.
Middle: Rescaled RoPE under linear positional interpolation. Down: LongRoPE fully exploits the identified two non-uniformities, leading
to varied interpolation and extrapolation across RoPE dimensions at different token positions.
Table 1. Perplexity of LLaMA2-7B extended via different methods.
By a simple search for the rescale factors of each RoPE dimension,
we can greatly reduce the perplexity.
(LLaMA2-7B)
Extension method
PI
Dy-NTK
YaRN
Search for RoPE Dim-wise Œª

Table 2. Perplexity of LLaMA2-7B extended on PG19 (5 samples).
When retaining the first nÃÇ tokens without positional interpolation,
the performance of both PI and Dynamic-NTK are improved.
(LLaMA2-7B)
L‚Ä≤
Extension method
8k
PI
16k
8k
Dy-NTK
16k

Context Window Size
PG19 (5 samples) Proof-pile (10 samples)
8192
16384
8192
16384
10.65
20.49
3.65
4.93
10.21
23.29
3.50
3.87
32.64
87.89
3.49
3.25
9.37
11.34
3.45
3.13

0
10.65
20.49
10.21
23.29

No interpolation for first nÃÇ tokens
2
4
8
16
32
64 128
10.65 10.65 10.65 10.66 10.59 10.49 10.54
20.39 20.36 20.02 19.64 19.96 20.64 22.27
10.21 10.21 10.21 10.22 10.20 10.15 10.13
23.29 23.27 23.08 22.68 22.94 23.58 27.94

256
11.14
34.65
10.62
90.99

Table 3. Proof-pile perplexity of the extended LLaMA2-7B with a
64k context window in non-fine-tuned and fine-tuned settings.

lower (high frequency) dimensions less and higher (low
frequency) dimensions more, resulting in both positional interpolation and extrapolation, where Œª = si . The improved
dynamic NTK (LocalLLaMA, 2023a) adjusts the extension
ratio at each position based on the current sequence length.
Unlike PI, which necessitates fine-tuning, NTK-aware methods can extend context windows in non-fine-tuning scenarios, but usually with a maximum extension ratio of 4√ó.

Method
PI
YaRN
Search (Dim-wise Œª and nÃÇ)

non-fine-tuned
72.54
4.15
3.22

fine-tuned
2.44
2.42
2.36

extrapolation. However, current non-linearities heavily rely
on human-designed rules. This naturally raises two questions: (1) Is the current positional interpolation optimal? (2)
Are there unexplored non-linearities?

YaRN (Peng et al., 2023) introduces a significant improvement to positional interpolation performance. It divides
RoPE dimensions into three frequency-based groups, each
with a different interpolation strategy. High frequency dimensions undergo extrapolation (Œª=1), while low frequency
dimensions use linear interpolation (PI). The RoPE dimensions that fall in-between employs the NTK. The key of
YaRN lies in its grouping of RoPE dimensions, which currently depends on human-led empirical experiments. This
may result in sub-optimal performance for new LLMs.

To answer these questions, we use evolution search (see
Sec. 3) to discover better non-uniform positional interpolations for LLaMA2-7B. The search is guided by perplexity,
using 5 random samples from PG19 (Rae et al., 2019) validation set. Through our empirical analysis, we reveal the
following key findings.
Finding 1: RoPE dimensions exhibit substantial nonuniformities, which are not effectively handled by current
positional interpolation methods.

2.2. Study on Non-uniform Positional Interpolation

We search the optimal Œª for each RoPE dimension in Eq. 2.
Table 1 compares the perplexity of LLaMA2-7B under different methods on PG19 and Proof-pile (Azerbayev et al.,
2022) test sets, without fine-tuning. Our searched solution

Inspired by NTK and YaRN, we notice their gains from nonlinearity, specifically in considering different frequencies
across RoPE dimensions for specialized interpolation and

3

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

shows significant improvements, suggesting that current
linear (PI) and non-uniform (Dynamic-NTK and YaRN) interpolations are sub-optimal. Notably, YaRN underperforms
than PI and NTK on PG19, as it doesn‚Äôt reach the target context window length for non-fine-tuned LLM. For example,
YaRN‚Äôs perplexity spikes after 7k in an 8k context size.

Table 4. Search space for RoPE rescale factors. Tuples of three
values represent the lowest value, highest, and step size.
Non-uniformity Notation
RoPE dimension
Œªi
Starting tokens

nÃÇ

Search Space
(1.0, extension ratio s√ó1.25, 0.01)
{0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 64, 128, 256}

context window beyond 2 million tokens.

Through our search, the rescaled factors Œª in Eq. 2 become
non-uniform, differing from the fixed scale s in PI, NTK‚Äôs
formula calculation, and YaRN‚Äôs group-wise calculation.
These non-uniform factors significantly improve LLaMA2‚Äôs
language modeling performance (i.e., perplexity) for 8k and
16k context windows without fine-tuning. This is because
the resulting positional embedding effectively preserves the
original RoPE, especially key dimensions, thus reducing
LLM‚Äôs difficulty in distinguishing close token positions.

3.1. Problem Formulation
The two non-uniformities can lead to a vast solution space
and introduce complexities in optimization. To address
it, we frame the multidimensional non-uniform position
interpolation optimization problem as a search problem.
For a LLM targeting a context window size of L‚Ä≤ and lengthy
input documents X, where each x ‚àà X surpasses L‚Ä≤ in
token length, we denote the original rotary angle of the ith
dimension in RoPE embedding at token position n as Œ≤ni .
The optimization problem is then formulated as follows:
arg min L (LLM(RoPE, X)) , where

Finding 2: RoPE for the initial tokens in the input sequence
should be extrapolated with less interpolation.
For the initial nÃÇ tokens in input sequences, we hypothesize
that their RoPE should do less interpolation. This is because
they receive large attention scores, making them crucial to
attention layers, as observed in Streaming LLM (Xiao et al.,
2023) and LM-Infinite (Han et al., 2023). To verify this, we
extend the context window to 8k and 16k using PI and NTK,
keeping the first nÃÇ (0,2, ..., 256) tokens without interpolation.
When nÃÇ=0, it reverts to the original PI and NTK. Table 2
highlights two key observations: (1) retaining the starting
tokens without position interpolation indeed improves the
performance. (2) The optimal number of starting tokens, nÃÇ,
depends on the target extension length.

x‚ààX; |x|‚â•L‚Ä≤

h

RoPE( n) = ¬∑¬∑, cos I(ŒªÃÇi , nÃÇ) √ó




, sin I(ŒªÃÇi , nÃÇ) √ó

1

n < nÃÇ
n ‚â• nÃÇ

n
Œ≤i

n
Œ≤i



i
, ¬∑¬∑

i=0,¬∑¬∑, d
2 ‚àí1;
n‚àà[0,|x|);

(
where I(ŒªÃÇi , nÃÇ) =

1
Œªi

(3)
where we introduce a set of rescale factors, I(ŒªÃÇi , nÃÇ), to cover
the two forms of non-uniformities. ŒªÃÇi and nÃÇ denote the
non-uniformity of RoPE dimensions and token positions,
respectively. Specifically, we use I(ŒªÃÇi , nÃÇ) to rescale the
rotation angle for the ith RoPE dimension, where ŒªÃÇi is the
rescale factor and nÃÇ is token position threshold. For initial
nÃÇ-1 token positions, the rescale factor ŒªÃÇi will not take effect,
and the original RoPE rotary angle Œ≤ni is used. For tokens at
positions n ‚â• nÃÇ, the rescale factor is applied.

Finding 3: Non-uniform positional interpolation effectively
extends LLM context window in both fine-tuning and nonfine-tuning settings.
While we‚Äôve shown that our searched non-uniform position
interpolation significantly improves the extension performance at 8k and 16k without fine-tuning, longer extensions
require fine-tuning. As such, we fine-tune LLaMA2-7B
with our searched RoPE for a 64k context window size (see
Appendix for settings). As Table 3 shows, our method significantly outperforms PI and YaRN, both before and after
fine-tuning LLaMA2-7B. This is due to our effective use of
non-uniform positional interpolation, minimizing information loss and providing a better initialization for fine-tuning.

Given a target context window size of L‚Ä≤ , our objective
is to find the optimal rescale factors (I(ŒªÃÇ0 , nÃÇ), I(ŒªÃÇ1 , nÃÇ)
,...I(ŒªÃÇi , nÃÇ)...) from the 1st to dth RoPE dimension. As a result, the target LLM, with the rescaled RoPE, can achieve a
minimum next token prediction loss, L (i.e., the perplexity),
for input samples X with a token length of L‚Ä≤ .
3.2. Searching the Non-uniform Position Interpolation
To solve the problem in Eq. 3, we now introduce our simple
yet highly effective method, which searches for the optimal
RoPE rescale factors to fully exploit the multidimensional
non-uniformities in position embedding.

Summary. Our study uncovers two non-uniformities:
varying RoPE dimensions and token positions. Utilizing
these non-uniformities effectively in positional interpolation
greatly improves LLM context extension performance.

Search space. We design a large search space to include
the two non-uniformies. Table 4 illustrates the search space.
Specifically, we allow the search of a specialized rescale
factor for each dimension in RoPE embedding. To simply
search space design, we search Œªi and nÃÇ instead of searching
for I(ŒªÃÇi , nÃÇ), where ŒªÃÇi = 1/Œªi . As shown in Table 4, Œªi is

3. LongRoPE
Motivated by the findings, we present LongRoPE, which
first introduces an efficient search algorithm to fully exploit
the two non-uniformities, and then uses it to extend LLM
4

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

Algorithm 1 The search algorithm

allowed to search from a minimum value of 1.0 (i.e., direct
extrapolation) to a maximum value of s √ó 1.25 (i.e., larger
interpolation than PI) with a step size of 0.01, where s is the
target context window extension ratio.

Input: target LLM, input samples X, population size P , mutation
size N1 , crossover size N2 , max iterations T , mutate probability p
1: Top-k=œï;
2: P0 =Initialize population with optimization (P , X, p);
3: for i=1 to T do
4: Compute perplexity (LLM, Pi‚àí1 , X);
5:
Top-k = Update Topk (Top-k, Pi‚àí1 );
6:
Pmutation =Mutation with mono constraint (Top-k, N1 , p);
7:
Pcrossover =Crossover with mono constraint (Top-k, N2 );
8:
Pi = Pmutation ‚à™ Pcrossover ‚à™ Top-k;
9: end for
10: Return the individual with lowest perplexity in Top-k;

nÃÇ controls the number of initial token positions that are
retained without position interpolation (i.e., use the original
RoPE embedding). Empirically, we allow nÃÇ to search from
{0, 1, 2, 4, 8, 12, 16, 20, 24, 28, 32, 64, 128, 256}. When
nÃÇ = 0, all token positions use the searched rescale factors.
Evolution-based search. Our search space in Table 4 spans
numerous positional interpolation solutions, posing a significant challenge for efficient exploration. For example, a
s = 4√ó extension leads to 400128/2 √ó 14=4√ó10167 choices.
With larger extension ratio, the search space expands exponentially. To address this, we use evolution search (Guo
et al., 2020) and introduce two optimization techniques to
greatly boost search efficiency. Algorithm 1 illustrates the
overall search procedure.
Optimized initial population generation. Instead of initializing a population of P rescale factors randomly, we add
the three RoPE rescale factors corresponding to PI, NTK,
and YaRN as individuals into the initial population. For the
remaining P -3 individuals, we randomly mutate the three
rescale factors with a probability of p.

Figure 3. LLaMA2-7B perplexity on PG19 and Proof-Pile after
extension using different methods, measured without fine-tuning.
By fully exploiting the non-uniformities, LongRoPE achieves an
8√ó extension without fine-tuning.

Monotonically non-decreasing constraint. After generating
the initial population, we compute LLM perplexity for each
individual. Specifically, we apply the corresponding RoPE
rescale factors to the target LLM and compute the perplexity of input X. The top-k individuals become parents for
evolution. However, the vast search space can cause naive
mutation and crossover to explore poor solutions, leading
to unnecessary perplexity computations. This is particularly inefficient when L‚Ä≤ is large, given the time-consuming
inference of each perplexity calculation.

3.3. Extending LLM Context Window to 2048K
Progressive extension to 2048k. We now introduce our
method to extend the context window of pre-trained LLMs
from the traditional 4k to over 2048k. As demonstrated,
our non-uniform positional interpolation can achieve 8√ó
extension without fine-tuning. For larger extensions (i.e.,
512√ó) is required, fine-tuning is necessary. One method is
to search for RoPE rescaled factors under the target 2048k
size and then fine-tune. However, this faces challenges due
to the prohibitively expensive training resources. Moreover,
based on our experiences, it‚Äôs challenging to well fine-tune
the LLMs under a large extension ratio (see Appendix).

methods such as PI, and non-uniform NTK and YaRN cause
perplexity to spike after 2√ó extension.

To address this, we impose a non-decreasing monotonicity
constraint on the sampled RoPE rescaled factors: Œªi ‚â§ Œªi+1 .
Only RoPE that satisfies this constraint is applied to LLM
for perplexity evaluation, significantly reducing the search
costs. Specifically, we require that Œªi increases monotonically with the RoPE dimension (i.e., i=0,...,63). This dimensional monotonicity is based on the NTK theory (Jacot et al.,
2018; Tancik et al., 2020; LocalLLaMA, 2023b), suggesting
that lower dimensions with higher frequency requires less
interpolation (i.e., a smaller Œªi ), and higher dimensions with
lower frequency can do more interpolation (i.e., a larger Œªi ).

Fortunately, LongRoPE is effective for both the original
and fine-tuned extended LLM. Therefore, we introduce an
efficient, progressive method that achieves the target 2048k
with just 1k fine-tuning steps at within 256k training length.
‚ô¢ Extending pre-trained LLM to 256k with LongRoPE
search. Taking LLaMA2 as an example, we conduct search
for target context window size of 128k and 256k. The extension ratio at this stage is 32√ó and 64√ó, respectively.
‚ô¢ Fine-tuning to 256k. Then, we fine-tune the pre-trained
LLM to achieve the context window size of 256k. Specifically, we first fine-tune LLaMA2 for 400 steps using the
RoPE rescaled factors for 128k. Then, we replace the RoPE
rescaled factors to 256k on the finished checkpoint and conduct an additional 600 steps of fine-tuning. This method

8√ó extension without fine-tuning. Our evolutionary
search effectively identifies non-uniform RoPE rescale factors, preserving key dimensions and positions to minimize
interpolation-induced information loss. As depicted in Fig.3,
our method is able to extend LLaMA2‚Äôs context window
from 4k to 32k without fine-tuning. In contrast, existing
5

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

proves more efficient than directly fine-tuning to 256k.

and crossover sizes. Perplexity is measured on 3 random
samples from Pile-Books3 (Gao et al., 2020) validation set.

‚ô¢ Extending fine-tuned extended LLM to 2048k with LongRoPE search. Finally, we perform a secondary search on
the fine-tuned 256k-length LLM. This ultimately results in
an extremely large context window size of 2048k without
further fine-tuning. The final extension ratio is 512√ó.

Baselines. To reach 2048k, we fine-tuned models with 128k
and 256k context windows. This yields LongRoPE-2048k
(ft=128k) and LongRoPE-2048k (ft=256k) for LLaMA2
and Mistral, respectively. We compare the four models
with state-of-the-art context window extension baselines,
specifically open-sourced LLMs fine-tuned after positional
interpolation using PI, NTK and YaRN. This includes
Together-32k (Together, 2023), Code LLaMA (RozieÃÄre
et al., 2023), LongLoRA-full-FT-100k (Chen et al., 2023b),
YaRN-LLaMA and YaRN-Mistral (Peng et al., 2023).

Shorter context window recovery. After extending to an
extremely long 2048k context window, we notice a performance drop within the original context window. This is a
known issue of positional interpolation (Chen et al., 2023a),
as it forces position embedding in higher dimensions within
the original context window to reside in a much narrower
region, negatively affecting the language model‚Äôs performance. With a 512√ó extension ratio, positions within the
original 4k context window become particularly crowded.

4.2. Main Results
Long sequence language modeling within 256k. We begin
by comparing with state-of-the-art extended LLMs within
a 256k evaluation length. We use two datasets to demonstrate our generalizability: Proof-pile (Rae et al., 2019) and
PG19 (Gao et al., 2020) test splits. We evaluate perplexity
at various context lengths using sliding window of 256. For
PG19, we use the whole test split of 100 documents. For
Proof-pile, we follow YaRN (Peng et al., 2023) to randomly
select 10 samples, each with at least 128k lengths.

To mitigate this, we perform an extra evolution search on
the extended LLM to adjust RoPE rescale factors for short
context lengths (e.g., 4k and 8k). We reduce the maximum allowed searched Œª due to less positional interpolation
required for shorter lengths. During inference, the LLM dynamically adjusts the corresponding RoPE rescale factors.

4. Experiments
4.1. Setup

Table 5 and Table 7 compare the perplexity of LLaMA2
and Mistral extended via different interpolation methods on
Proof-pile and PG19, respectively. We highlight two key
observations: (1) our extended models show an overall decreasing perplexity trend from 4k to 256k evaluation lengths,
proving their abilities to leverage longer context. (2) Even
with a context window 16√ó longer, a condition typically
challenging for maintaining performance at shorter lengths,
our LongRoPE-2048k models outperform state-of-the-art
baselines within 256k context length.

Evaluation Tasks and models. We apply LongRoPE on
LLaMA2-7B and Mistral-7B, and evaluate the performance
on three aspects: (1) perplexity of extended-context LLMs
on long documents; (2) Passkey retrieval task that measures
a model‚Äôs ability to retrieve a simple passkey from a sea of
irrelevant text; and (3) Standard LLM benchmarks within a
short 4096 context window size.
Fine-tuning. For LLaMA2, we use a learning rate of 2e-5
with linear decay and a global batch size of 32. We finetune for 400 steps on Redpajama (Computer, 2023) dataset,
chunked into 128k segments bookended with the BOS and
EOS tokens. Then, based on the finished checkpoint, we
train an additional 600 steps to achieve 256k context window.
The 128k context size is trained on 8 A100 GPUs with the
distributed training system (Lin et al., 2023), while the 256k
requires 16 A100 GPUs. In the case of Mistral, a constant
learning rate of 1e-6 and a global batch size of 64 are used.
For both 128k and 256k models, we follow the setting in
YaRN (Peng et al., 2023), with 400 steps on the Together
Computer‚Äôs Long-Data Collections (mis, 2024) using 16k
sequence length. We use 4 A100 GPUs for training.

Long sequence language modeling beyond 2000k. To
evaluate the effectiveness on extremely long documents, we
use the Books3 (Gao et al., 2020) dataset. For evaluation
efficiency, we randomly select 20 books, each exceeding
2048k in length, and use a sliding window of 256k.
As shown in Table 6, LongRoPE successfully extends
LLaMA2-7B and Mistral-7B‚Äôs context window to 2048k,
while also achieving perplexity comparable or superior to
baselines within shorter lengths of 8k-128k. We also observe notable performance differences between the 2048k
LLaMA2 and Mistral. Mistral outperforms baselines at
shorter lengths, but perplexity exceeds 7 beyond 256k.
LLaMA2 performance aligns with expectations: the perplexity decreases gratefully with longer contexts, with marginal
increases at 1024k and 2048k. Moreover, on LLaMA2,
LongRoPE-2048k performs better at a fine-tuning length
of 256k over 128k, due to the smaller secondary extension
ratio (i.e., 8√ó vs. 16√ó). In contrast, Mistral performs better

Search. For target window size within 256k, we use:
P =64, N1 =N2 =16, p=0.3, T =40, and select top-32 for
mutation/crossover in each iteration. Perplexity is calculated using 5 random PG19 validation set samples, with a
minimum length requirement of the target context length.
For windows over 512k, we halve the population, mutation,
6

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
Table 5. Proof-pile perplexity of models with various positional interpolation methods. ft: the context window size used in fine-tuning.
Even with a context window 16√ó longer than current long-context models, our models also outperform them within 256k context length.
Base
LLM

LLaMA2-7B

Mistral-7B

Model
Name
LLaMA2-7B
Together
LongLoRA
Code LLaMA
YaRN (s=16)
YaRN (s=32)
LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)
Mistral v0.1
YaRN (s=8)
YaRN (s=16)
LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)

Context
Window
4k
32k
100k
100k
64k
128k
2048k
2048k
8k
64k
128k
2048k
2048k

Extension
Method
PI
PI
NTK
YaRN
YaRN
LongRoPE
LongRoPE
YaRN
YaRN
LongRoPE
LongRoPE

4096
3.58
3.69
3.83
3.95
3.69
3.75
3.71
3.85
3.09
3.18
3.21
3.20
3.20

8192
>104
3.50
3.62
3.71
3.51
3.56
3.50
3.65
2.96
3.04
3.08
3.04
3.04

Evaluation Context Length
32768
65536
98304
>104
>104
>104
2.64
>102
>103
2.68
2.44
2.33
2.74
2.55
2.54
2.65
2.42
>101
2.70
2.45
2.36
2.60
2.36
2.27
2.63
2.38
2.28
>102
>103
>103
2.37
2.20
10.39
2.41
2.24
2.18
2.36
2.18
2.13
2.36
2.18
2.13

131072
>104
>104
9.89
2.71
>101
2.37
2.26
2.26
>103
57.4
2.19
2.13
2.14

262144
>104
>104
>103
49.33
>104
99.64
1.88
1.87
>104
>104
4.91
1.85
1.84

Table 6. Perplexity evaluation on Books3 dataset. Without additional fine-tuning, our LongRoPE-2048k models, with a training context
window size of 128k and 256k, effectively scale to an extremely long context size of 2048k. 1k=1024 tokens.
Base
LLM

Model
Name
LongLoRA
Code LLaMA
YaRN (s=16)
LLaMA2-7B
YaRN (s=32)
LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)
Mistral v0.1
YaRN (s=16)
Mistral-7B
YaRN (s=32)
LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)

Context
Window
100k
100k
64k
128k
2048k
2048k
8k
64k
128k
2048k
2048k

Extension
Method
PI
NTK
YaRN
YaRN
LongRoPE
LongRoPE
YaRN
YaRN
LongRoPE
LongRoPE

8k
6.99
7.68
6.33
6.38
6.55
6.81
6.32
6.59
6.70
6.64
6.63

16k
6.80
7.49
6.20
6.25
6.35
6.66
66.61
6.48
6.63
6.48
6.48

32k
6.66
7.38
6.11
6.16
6.24
6.31
>102
6.42
6.65
6.39
6.38

Evaluation Context Length
64k
128k
256k
6.59
20.57
246.45
7.88
9.80
98.30
6.06
>104
>104
6.11
6.12
> 104
6.18
6.17
6.17
6.27
6.21
6.17
>103
>103
>103
6.45
104.15
727.20
6.72
6.85
99.90
6.45
6.64
7.08
6.43
6.68
7.15

512k
>103
>103
>104
>104
6.36
6.17
> 103
> 103
7.71
7.98

1024k
>104
>104
>104
>104
6.83
6.35
> 104
> 104
8.93
9.42

2048k
>104
>104
>104
>104
7.80
7.08
> 104
> 104
12.78
13.71

Table 7. Perplexity evaluation within 256k context length on PG19.
Base
LLM

Model
Name

LongLoRA
Code LLaMA
LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)
YaRN-64k
YaRN-128k
Mistral-7B LongRoPE-2048k (ft=128k)
LongRoPE-2048k (ft=256k)

LLaMA2-7B

Context Extension
Window Method
100k
100k
2048k
2048k
64k
128k
2048k
2048k

PI
NTK
LongRoPE
LongRoPE
YaRN
YaRN
LongRoPE
LongRoPE

Evaluation Context Length
8k
64k
128k
7.16
7.58
6.98
7.37
7.12
7.30
7.13
7.10

6.81
8.92
6.59
6.64
7.17
7.53
7.01
6.98

> 103
16.80
6.35
6.31
> 103
7.32
7.02
7.13

Figure 4. Passkey retrieval accuracy of long-context LLMs. It
showcases the remarkable ability of our models to accurately retrieve a passkey from a vast pool of million-level tokens.

at fine-tuning window size of 128k. The main reason is that
for Mistral‚Äôs 128k and 256k fine-tuning, we follow YaRN‚Äôs
setting to use a 16k training length, which affects Mistral‚Äôs
ability to further extend context window after fine-tuning.

trieving a passkey from million-level tokens, our LongRoPELLaMA2-2048k (ft=256k) manage to maintain a high retrieval accuracy (‚â•90%) from 4k to 2048k. LongRoPEMistral-2048k (ft=128k) keeps 100% accuracy up to 1800k,
dropping to 60% at 2048k, aligning with expectations from
Table 6, where the perplexity slightly increases at 2048k.

Passkey retrieval. We now study the effective context
window size in generation tasks. We follow a synthetic evaluation task of passkey retrieval proposed by (Mohtashami
& Jaggi, 2023). In this task, the model is asked to retrieve
a random passkey (i.e., a five-digit number) hidden in long
document. The prompt template is detailed in appendix. We
perform 10 iterations of the passkey retrieval task with the
passkey placed at a random location uniformly distributed
across the evaluation context length.

Standard benchmarks within original context window.
We evaluate LongRoPE-2048k models on the original
context window using Hugging Face Open LLM Leaderboard (Face, 2024) in zero-shot and few-shot settings. We
use 25-shot ARC-Challenge (Clark et al., 2018). 10-shot
HellaSwag (Zellers et al., 2019), 5-shot MMLU (Hendrycks
et al., 2020), and 0-shot TruthfulQA (Lin et al., 2021).

Fig. 4 shows the retrieval accuracy comparison with baselines. Existing models‚Äô accuracy rapidly drops to 0 beyond
128k. In contrast, despite the very challenging task of re7

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
Table 8. Comparison of long-context LLMs with original LLaMA2
and Mistral on the Hugging Face Open LLM benchmark.

Table 10. Ablation study on LongRoPE readjustment for performance recovery at shorter context lengths.
With
Recovery
√ó
LLaMA2-7B-2048k (ft=128k)
‚úì
√ó
LLaMA2-7B-2048k (ft=256k)
‚úì

(a) LLaMA2-7B with extended context window
Context
ARC-c HellaSwag MMLU TruthfulQA
Window

FT Model

Model

Original LLaMA2-7B
4k
Together
32k
Code LLaMA
100k
YaRN (s=16)
64k
YaRN (s=32)
128k
LongRoPE-2048k (ft=128k) 2048k
LongRoPE-2048k (ft=256k) 2048k

53.1
47.6
42.4
52.4
52.2
52.9
51.0

78.6
76.1
64.8
78.7
78.5
76.5
75.3

46.6
43.3
40.1
42.4
41.8
43.4
39.6

(b) Mistral-7B with extended context window
Original Mistral-7B
8k
60.6
83.2
63.6
MistralLite (Amazon, 2023) 16k
59.2
81.6
50.4
YaRN (s=16)
64k
59.3
81.3
61.3
YaRN (s=32)
128k
59.0
80.5
60.5
LongRoPE-2048k (ft=128k) 2048k
59.0
81.2
61.3
LongRoPE-2048k (ft=256k) 2048k
59.2
80.9
61.1

39.0
39.2
37.1
38.2
37.4
38.8
37.3

Table 11. Ablation study on the two forms of non-uniformities.
LLaMA2-7B
PG19 Perplexity
16k
32k
Linear interpolation (PI)
14.88 136.30
RoPE dim (Ours)
7.28
13.00
RoPE dim+Start tokens (Ours) 7.22
11.51
Methods

42.6
38.3
42.5
42.5
43.1
42.2

PI
YaRN
LongRoPE

6.60
6.39
6.17

8.73
6.79
6.35

LLaMA2-7B (ft=256k)
Books3 Perplexity
2048k
20.17
7.08
7.08

contributes to the performance. We setup two experiments:
(i) extending LLaMA2-7B to short 16k and 32k using different methods‚ÄîPI, searching for RoPE dimension only, and
searching for both non-uniformities; (ii) extending our finetuned 256k-length LLaMA2 to 2048k following the same
procedure. The perplexity is evaluated without fine-tuning.
As Table 11 shows, non-uniformity in RoPE dimension significantly reduces perplexity compared to PI‚Äôs linear interpolation. Non-uniformity in token position clearly improves
performance at 16k and 32k lengths but does not show the
same impact at 2048k, possibly due to the extremely long
length. Preserving only the initial tokens without interpolation becomes non-useful, and we leave this as future work.

Table 9. Books3 perplexity comparison of extending LLaMA2256k via different secondary positional interpolation methods.
Model
Extension
Context Window Size
Name
Method
512k 1024k 2048k
LLaMA2-7B (ft=256k)

Proof-Pile Perplexity LLM Benchmark
4k
8k
Avg. Accuracy
4.16
3.72
49.3
3.71
3.50
52.9
4.51
3.82
47.9
3.85
3.65
50.8

20.17
8.27
7.08

As Table 8 shows, our models achieve comparable results
on the original benchmark designed for a smaller context
window, and even outperform the original Mistral on TruthfulQA by +0.5%. LongRoPE-LLaMA2-2048k, fine-tuned
at 256k, shows slightly more performance degradation, but
remains within reasonable ranges for most tasks.

5. Related Works
In addition to methods based on position interpolation, this
section discusses related works of other approaches.

4.3. Ablation Results
Effectiveness of the second positional interpolation. In
our progressive extension strategy, we use our search algorithm to conduct a second non-uniform positional interpolation on the fine-tuned extended LLMs. We validate
its effectiveness by running experiments on our fine-tuned
LLaMA2-256k model. We extend it to 512k, 1024k, and
2048k using PI and YaRN. As Table 9 shows, our nonuniform positional interpolation sustains a consistent level
of perplexity. In contrast, the perplexity under PI and YaRN
quickly increases with the extension ratio.

Retrieval-based approaches use an external memory module to memorize long past context and retrieval modules for
related documents fetching at inference (Tworkowski et al.,
2023; Wang et al., 2023; Borgeaud et al., 2022). These
designs typically need explicit modifications on the LLM
architectures. Our work, in contrast, is more lightweight,
with minor positional embedding modifications. We can
also handle more long context tasks beyond retrieval, such
as long document summarization and few-shot learning.
Attention-based context window extensions. Beyond positional embedding interpolation, some research achieves
input context extension using the original LLM context window length by manipulating attention mechanisms (Han
et al., 2023; Xiao et al., 2023; Ratner et al., 2022). The key
idea is to mitigate the attention explosion issue caused by
new positions using novel attention masks. These efforts
and positional interpolation methods are complementary.

Effectiveness of recovery at shorter context lengths. To
mitigate performance loss at shorter context lengths, we
readjust the RoPE factors for LongRoPE-2048k via our
search algorithm. Specifically, we decrease the maximum
allowable scale factors for the search to encourage less interpolation at short 4k and 8k lengths. Table 10 shows the
perplexity comparison of LongRoPE-LLaMA2-2048k on
Proof-pile at 4k and 8k lengths, along with the average LLM
benchmark accuracy. The results clearly demonstrate a significant performance improvement at short context lengths.

Fine-tuning based approaches focus on how to effectively
fine-tune pre-trained LLMs with modified position embeddings for longer context. Works like Code LLaMA (RozieÃÄre
et al., 2023), LLaMA2 Long (Xiong et al., 2023) and Scale-

Analysis on the two forms of non-uniformities. Finally,
we ablate on the two non-uniformities to see how each part
8

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

dRoPE (Liu et al., 2023) choose a very large base value
for RoPE and fine-tune on the target length. Our method
offers flexibility for various target lengths and can achieve
beyond 2M length. More recently, as fine-tuning for long
context lengths (i.e., over 128k) demands substantial GPU
resources, LongLoRA (Chen et al., 2023b) and PoSE (Zhu
et al., 2023) are proposed to mitigate this overhead. Our
method is orthogonal to these efficient fine-tuning works.

context window of large language models via positional
interpolation. arXiv preprint arXiv:2306.15595, 2023a.
Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and
Jia, J. Longlora: Efficient fine-tuning of long-context
large language models. arXiv:2309.12307, 2023b.
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
Schoenick, C., and Tafjord, O. Think you have solved
question answering? try arc, the ai2 reasoning challenge.
arXiv:1803.05457v1, 2018.

6. Conclusion
In this work, we present LongRoPE, a method that remarkably extends the context length of LLMs to an unprecedented 2048k, while maintaining their capabilities within
original shorter context window. We exploit two forms of
non-uniformities in RoPE positional embedding using an
efficient evolutionary search. This offers twofold benefits:
it provides good initialization for fine-tuning and enables an
8√ó context window extension without fine-tuning. Building
on this, we propose a progressive extension strategy using
256k-length fine-tuned LLMs to reach a 2048k context window size without extra fine-tuning. Extensive experiments
validate the effectiveness of LongRoPE. We envision that
our LongRoPE-2048k models will enable many new long
context applications and inspire further research.

Computer, T.
Redpajama: An open source recipe
to reproduce llama training dataset, 2023.
URL
https://github.com/togethercomputer/
RedPajama-Data.
Dao, T. FlashAttention-2: Faster attention with better parallelism and work partitioning. 2023.
Face, H.
Open llm leaderboard, 2024.
URL
https://huggingface.co/spaces/
HuggingFaceH4/open_llm_leaderboard.
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,
Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,
Presser, S., and Leahy, C. The Pile: An 800gb dataset
of diverse text for language modeling. arXiv preprint
arXiv:2101.00027, 2020.

Broader Impacts
This paper presents work whose goal is to advance the field
of Machine Learning. There are many potential societal
consequences of our work, none which we feel must be
specifically highlighted here.

Guo, Z., Zhang, X., Mu, H., Heng, W., Liu, Z., Wei, Y., and
Sun, J. Single path one-shot neural architecture search
with uniform sampling. In Computer Vision‚ÄìECCV 2020:
16th European Conference, Glasgow, UK, August 23‚Äì28,
2020, Proceedings, Part XVI 16, pp. 544‚Äì560. Springer,
2020.

References

Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.
Lm-infinite: Simple on-the-fly length generalization for
large language models. arXiv preprint arXiv:2308.16137,
2023.

Long-data collections, 2024.
URL https:
//huggingface.co/datasets/
togethercomputer/RedPajama-Data-1T.
Amazon.
Mistrallite, 2023.
URL https://
huggingface.co/amazon/MistralLite.

Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
M., Song, D., and Steinhardt, J. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300, 2020.

Azerbayev, Z., Ayers, E., and Piotrowski, B. Proofpile, 2022.
URL https://github.com/
zhangir-azerbayev/ProofNet.

Huang, X., Zhang, L. L., Cheng, K.-T., Yang,
F., and Yang, M.
Fewer is more:
Boosting llm reasoning with reinforced context pruning.
2023. URL https://api.semanticscholar.
org/CorpusID:266210460.

Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford,
E., Millican, K., Van Den Driessche, G. B., Lespiau, J.-B.,
Damoc, B., Clark, A., et al. Improving language models
by retrieving from trillions of tokens. In International
conference on machine learning, pp. 2206‚Äì2240. PMLR,
2022.

Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks.
Advances in neural information processing systems, 31,
2018.

Chen, S., Wong, S., Chen, L., and Tian, Y. Extending

9

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel,
G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,
T., and Sayed, W. E. Mistral 7b, 2023.

S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus,
L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L.,
Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G.,
Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,
Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy, C., Han,
J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse,
C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B.,
Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S.,
Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S.,
Jonn, B., Jun, H., Kaftan, T., ≈Åukasz Kaiser, Kamali, A.,
Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L.,
Kim, J. W., Kim, C., Kim, Y., Kirchner, H., Kiros, J.,
Knight, M., Kokotajlo, D., ≈Åukasz Kondraciuk, Kondrich,
A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V.,
Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D.,
Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T.,
Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning,
S., Markov, T., Markovski, Y., Martin, B., Mayer, K.,
Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C.,
McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick,
J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V.,
Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O.,
MeÃÅly, D., Nair, A., Nakano, R., Nayak, R., Neelakantan,
A., Ngo, R., Noh, H., Ouyang, L., O‚ÄôKeefe, C., Pachocki,
J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo,
G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng,
A., Perelman, A., de Avila Belbute Peres, F., Petrov, M.,
de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M.,
Pong, V., Powell, T., Power, A., Power, B., Proehl, E.,
Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C.,
Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez,
H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S.,
Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker,
S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin,
J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang,
J., Tezak, N., Thompson, M., Tillet, P., Tootoonchian,
A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J.
F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright,
C., Wang, J. J., Wang, A., Wang, B., Ward, J., Wei, J.,
Weinmann, C., Welihinda, A., Welinder, P., Weng, J.,
Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich,
S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M.,
Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba,
W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng,
T., Zhuang, J., Zhuk, W., and Zoph, B. Gpt-4 technical
report, 2023.

Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring
how models mimic human falsehoods. arXiv preprint
arXiv:2109.07958, 2021.
Lin, Z., Miao, Y., Liu, G., Shi, X., Zhang, Q., Yang, F.,
Maleki, S., Zhu, Y., Cao, X., Li, C., et al. Superscaler:
Supporting flexible dnn parallelization via a unified abstraction. arXiv preprint arXiv:2301.08984, 2023.
Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
Scaling laws of rope-based extrapolation. arXiv preprint
arXiv:2310.05209, 2023.
LocalLLaMA.
Dynamically scaled rope further increases performance of long context
llama with zero fine-tuning, 2023a.
URL
https://www.reddit.com/r/LocalLLaMA/
comments/14mrgpr/dynamically_scaled_
rope_further_increases/.
LocalLLaMA. Ntk-aware scaled rope allows llama models
to have extended (8k+) context size without any finetuning and minimal perplexity degration, 2023b. URL
https://www.reddit.com/r/LocalLLaMA/
comments/14lz7j5/ntkaware_scaled_
rope_allows_llama_models_to_have/.
Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L.,
Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang,
Y., Gupta, S., Majumder, B. P., Hermann, K., Welleck,
S., Yazdanbakhsh, A., and Clark, P. Self-refine: Iterative
refinement with self-feedback, 2023.
Mohtashami, A. and Jaggi, M. Landmark attention:
Random-access infinite context length for transformers.
arXiv preprint arXiv:2305.16300, 2023.
OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J.,
Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M.,
Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G.,
Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman,
A.-L., Brockman, G., Brooks, T., Brundage, M., Button,
K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson,
C., Carmichael, R., Chan, B., Chang, C., Chantzis, F.,
Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess,
B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N.,
Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning,

Park, J. S., O‚ÄôBrien, J., Cai, C. J., Morris, M. R., Liang,
P., and Bernstein, M. S. Generative agents: Interactive
simulacra of human behavior. In Proceedings of the 36th
Annual ACM Symposium on User Interface Software and
Technology, pp. 1‚Äì22, 2023.

10

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

Peng, B., Quesnelle, J., Fan, H., and Shippole, E. Yarn:
Efficient context window extension of large language
models. arXiv preprint arXiv:2309.00071, 2023.

V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,
Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,
I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,
K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,
Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,
M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,
and Scialom, T. Llama 2: Open foundation and fine-tuned
chat models, 2023.

Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C.,
and Lillicrap, T. P. Compressive transformers for longrange sequence modelling. arXiv preprint, 2019. URL
https://arxiv.org/abs/1911.05507.
Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend,
O., Karpas, E., Shashua, A., Leyton-Brown, K., and
Shoham, Y. Parallel context windows improve in-context
learning of large language models. arXiv preprint
arXiv:2212.10947, 2022.

Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y.,
Michalewski, H., and Mi≈ÇosÃÅ, P. Focused transformer:
Contrastive training for context scaling. 2023.

RozieÃÄre, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan,
X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov,
A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C.,
Grattafiori, A., Xiong, W., DeÃÅfossez, A., Copet, J., Azhar,
F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and
Synnaeve, G. Code llama: Open foundation models for
code, 2023.

Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J.,
and Wei, F. Augmenting language models with long-term
memory. arXiv preprint arXiv:2306.07174, 2023.

Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu,
Y. Roformer: Enhanced transformer with rotary position
embedding. arXiv preprint arXiv:2104.09864, 2021.

Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P.,
Hou, R., Martin, L., Rungta, R., Sankararaman, K. A.,
Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S.,
Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M.,
Wang, S., and Ma, H. Effective long-context scaling of
foundation models, 2023.

Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks.
arXiv, 2023.

Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil,
S., Raghavan, N., Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R. Fourier features let networks learn
high frequency functions in low dimensional domains.
Advances in Neural Information Processing Systems, 33:
7537‚Äì7547, 2020.

Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi,
Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the
Association for Computational Linguistics, 2019.

Together, 2023. URL https://huggingface.co/
togethercomputer/LLaMA-2-7B-32K.

Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou,
Z. Soaring from 4k to 400k: Extending llm‚Äôs context
with activation beacon. arXiv preprint arXiv:2401.03462,
2024.

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,
M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,
Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,
A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,

Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and
Li, S. Pose: Efficient context window extension of llms
via positional skip-wise training, 2023.

11

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

A. Appendix
A.1. Settings
Environments. All our experiments are conduct on 16 A100 GPUs. We employ Flash Attention-2 (Dao, 2023) to accelerate
both training and inference. As the GPU memory and computation time increase exponentially with the sequence length,
it‚Äôs challenging to serve the fine-tuning and inference with context length beyond 512k. As a result, we utilize an internal
platform, CUBE - an internal version of (Lin et al., 2023), to reduce both the training and inference costs.
Passkey prompt. We follow existing literature (Mohtashami & Jaggi, 2023; Chen et al., 2023a; Peng et al., 2023; Chen
et al., 2023b; Zhu et al., 2023) for the document format of passkey retrieval. We show the prompt template as follows:
There is an important info hidden inside a lot of irrelevant text. Find it and
memorize them. I will quiz you about the important information there.
The grass is green. The sky is blue.
back again. (repeat x times)
The pass key is 17865.

Remember it.

The grass is green. The sky is blue.
back again. (repeat y times)
What is the pass key?

The sun is yellow.

Here we go.

There and

17865 is the pass key.
The sun is yellow.

Here we go.

There and

The pass key is

The document length varies with the value of x and y. 17865 is the passkey number to retrieve. It is randomly sampled and
varies at each testing time.
A.2. Additional details on fine-tuning
As introduced in Section 4.2, we fine-tune two context window lengths, namely 128k and 256k, for both LLaMA2 and
Mistral. Specifically, the model with a 256k context window begins its fine-tuning from the 128k-length checkpoint.
Fig. 5(ab) illustrates the training loss for LLaMA2 and Mistral during this fine-tuning process. We highlight three key
observations: (1) The model with a 128k context window experiences a large initial loss due to a 32√ó extension. However,
the loss rapidly decreases after a few steps. (2) LLaMA2 and Mistral employ different fine-tuning settings. Mistral achieves
the desired long context window by fine-tuning on 16k-length data, while LLaMA2 necessitates text lengths that match
the context window size. Furthermore, we adopt YaRN‚Äôs strategy of using a constant learning rate. As a result, it can be
observed that Mistral‚Äôs loss begins to fluctuate after dropping to around 2.2. (3) For both Mistral and LLaMA2, the model
with a 256k context window, which starts fine-tuning from the 128k checkpoint, exhibits a low initial training loss. This
suggests that fine-tuning from 128k-length checkpoints is effective and significantly facilitates convergence.

(a)

(b)

(c)

Figure 5. (ab): Loss curve in fine-tuning LLaMA2-7B and Mistral-7B with extended context window size. (c) The training loss of
fine-tuning LLaMA2-7B with a 256k context window under different fine-tuning settings.

We also explore different settings to fine-tune LLaMA2 with 256k context window. As shown in Fig. 5(c), we experiment
with two additional settings: (i) using the RoPE rescale factors corresponding to 256k, we directly fine-tune on LLaMA2-7B,
and (ii) using RoPE rescale factors for 256k, we fine-tune on LLaMA2-7B, but truncate the text lengths to 128k. The loss
12

LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens

curves are displayed in Fig. 5(c). We observe that using 128k text lengths to fine-tune a model with a 256k context window
results in a sharp increase in the initial loss. Directly fine-tuning from LLaMA2-7B to achieve 256k results in a relatively
slow decrease in loss. Table 12 shows the test perplexity on Proof-Pile for checkpoints from three different settings. This
indicates that our current approach of fine-tuning from a 128k-checkpoint is the most effective.
Table 12. Proof-pile perplexity of extended LLaMA2-7B via different fine-tuning settings. Tuples of three values represent the fine-tuning
text length, context window size and initial checkpoint.
Method
(fine-tune L‚Ä≤ , L‚Ä≤ , base LLM)

Evaluation Context Length
32768 65536 98304 131072 262144

(128k, 256k, LLaMA2-7B)
9.75
(256k, 256k, LLaMA2-7B)
4.51
(128k, 256k, LLaMA2-7B (ft=128k) 2.66

6.56
2.87
2.38

5.15
2.53
2.28

5.19
2.39
2.26

2.21
1.95
1.87

Fine-tuning cost. LLaMA2-128k uses 8 A100 GPUs for a week to fine-tune 400 steps. LLaMA2-256k doubles the
resources to 16 A100 GPUs for two weeks to fine-tune 600 steps. For Mistral-128k and 256k, with a training length of 16k,
we employ 4 A100 GPUs for a 2-day fine-tuning period.
A.3. Additional details on the search

(a)

(b)

(c)

Figure 6. Perplexity on the validation samples at each evolution search iteration. (a) The 64√ó extension for LLaMA2-7B to reach 256k
context window size. (b) The 8√ó extension for LLaMA2-7B-256k to reach 2048k context window size. (c) The 16√ó extension for
LLaMA2-7B-128k to reach 2048k context window size.

Search efficiency. Fig. 6 illustrates the perplexity on the validation samples at each evolution search iteration. We can
see that our search algorithm can efficiently find high-quality non-uniform RoPE rescale factors. Specifically, on the 256k
context window search (Fig. 6(a)), after the first iteration, we can find solutions significantly better than PI and YaRN. As
searching more iterations, we can significantly reduce the validation perplexity from 273.27 from 118.47. Furthermore,
we can observe that YaRN, as the previous state-of-the-art non-uniform interpolation method, performs even worse than
PI (linear interpolation) at the 64√ó extension. This also indicates that human-heuristic-based non-uniform interpolation is
challenging to perform well in all scenarios.
For the extremely long context window at 2048k, we use the fine-tuned 128k and 256k context window‚Äôs LLaMA2-7B
for 16√ó and 8√ó extension, respectively. As shown in Fig. 6(bc), as expected, the perplexity of the 16√ó extension is larger
than that of the 8√ó extension. Additionally, due to the time required for a single perplexity evaluation at 2048k is about 50
minutes, the search iterations are constrained. If more search time is allowed, it‚Äôs highly possible to search better results.
Search cost. The search cost is primarily depending on the time required to evaluate the perplexity of input context at a
given context window size. For context window lengths up to 256k, the total search time is relatively quick, achievable
within 3 days using a single A100 GPU. For a 512k context window, we employ 2 A100 GPUs. For larger context windows
of 1024k and 2048k, we utilize 4 and 8 A100 GPUs respectively, managing to keep the total search time within a 5-day limit.

13




Preprint. Under review.

Leave No Context Behind:
Efficient Infinite Context Transformers with Infini-attention
Tsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal
Google
tsendsuren@google.com

arXiv:2404.07143v2 [cs.CL] 9 Aug 2024

Abstract
This work introduces an efficient method to scale Transformer-based Large
Language Models (LLMs) to infinitely long inputs with bounded memory
and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates
a compressive memory into the vanilla attention mechanism and builds
in both masked local attention and long-term linear attention mechanisms
in a single Transformer block. We demonstrate the effectiveness of our
approach on long-context language modeling benchmarks, 1M sequence
length passkey context block retrieval and 500K length book summarization
tasks with 1B and 8B LLMs. Our approach introduces minimal bounded
memory parameters and enables fast streaming inference for LLMs.

1

Introduction

Memory serves as a cornerstone of intelligence, as it enables efficient computations tailored
to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based
LLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have
a constrained context-dependent memory, due to the nature of the attention mechanism.
The attention mechanism in Transformers exhibits quadratic complexity in both memory
footprint and computation time. For example,
Linear
the attention Key-Value (KV) states have 3TB
projection
memory footprint for a 500B model with batch
size 512 and context length 2048 (Pope et al.,
2023). Indeed, scaling LLMs to longer sequences
(i.e. 1M tokens) is challenging with the standard
Concat
Concat
Transformer architectures and serving longer
Retrieve
and longer context models becomes costly finanCompressive memory &
Causal scaled dot-product
cially.
Linear attention
attention & PE
Update
Compressive memory systems promise to be
V
V
Q
more scalable and efficient than the attention
V
V
Q
{KV}
{KV}
Q
mechanism for extremely long sequences (Kanerva, 1988; Munkhdalai et al., 2019). Instead
of using an array that grows with the input se- Figure 1: Infini-attention has an addiquence length, a compressive memory primarily tional compressive memory with linear
maintains a fixed number of parameters to store attention for processing infinitely long
and recall information with a bounded storage contexts. {KV }s‚àí1 and {KV }s are attenand computation costs. In the compressive mem- tion key and values for current and previory, new information is added to the memory ous input segments, respectively and Qs
by changing its parameters with an objective the attention queries. PE denotes position
that this information can be recovered back later embeddings.
on. However, the LLMs in their current state
have yet to see an effective, practical compressive memory technique that balances simplicity along with quality.
s-1

1

s

s

Preprint. Under review.

In this work, we introduce a novel approach that enables Transformer LLMs to effectively
process infinitely long inputs with bounded memory footprint and computation. A key
component in our proposed approach is a new attention technique dubbed Infini-attention
(Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention
mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) and builds in both masked local
attention and long-term linear attention mechanisms in a single Transformer block.
Such a subtle but critical modification to the Transformer attention layer enables a natural
extension of existing LLMs to infinitely long contexts via continual pre-training and finetuning.
Our Infini-attention reuses all the key, value and query states of the standard attention
computation for long-term memory consolidation and retrieval. We store old KV states of
the attention in the compressive memory, instead of discarding them like in the standard
attention mechanism. We then retrieve the values from the memory by using the attention
query states when processing subsequent sequences. To compute the final contextual
output, the Infini-attention aggregates the long-term memory-retrieved values and the local
attention contexts.
In our experiments, we show that our approach outperforms baseline models on longcontext language modeling benchmarks while having 114x comprehension ratio in terms of
memory size. The model achieves even better perplexity when trained with 100K sequence
length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval
task when injected with Infini-attention. Finally, we show that a 8B model with Infiniattention reaches a new SOTA result on a 500K length book summarization task after
continual pre-training and task fine-tuning.
In summary, our work makes the following contributions:
1. We introduce a practical and yet powerful attention mechanism ‚Äì Infini-attention
with long-term compressive memory and local causal attention for efficiently modeling both long and short-range contextual dependencies.
2. Infini-attention introduces minimal change to the standard scaled dot-product attention and supports plug-and-play continual pre-training and long-context adaptation
by design.
3. Our approach enables Transformer LLMs to scale to infinitely long context with a
bounded memory and compute resource by processing extremely long inputs in a
streaming fashion.

2

Background

Recurrent Neural Networks (RNNs) process a single token xt at each step t and computes a
recurrent hidden state ht to represent an entire input sequence (Hochreiter & Schmidhuber,
1997; Maass et al., 2002):
ht = RNN ( xt , ht‚àí1 ).
(1)
The RNN computation is very efficient since the model maintains only a fixed-size vector
ht for input sequence. However, for processing long sequences it becomes difficult to
store entire contextual information into a single fixed-size vector and this limitation had
implications on RNNs utility in certain tasks (Kaiser & Sutskever, 2015). To address the
limitation, people extended the standard RNNs with an external memory component that
can be read from and written to. One such an instance is Metalearned Neural Memory
(MNM) (Munkhdalai et al., 2019):
ht , Œ∏t = MN M ( xt , ht‚àí1 , Œ∏t‚àí1 ).

(2)

MNM learns an additional memory state Œ∏ parameterized by a feed-forward neural network
(FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar
to the attention mechanism. To store information, it modifies the parameters of the FFN
by using the key vectors as input and the value vectors for the target, and to read memory
2

Preprint. Under review.

Infini-Transformer

Transformer block:

Segment 1

Segment 2

Segment 3

Compressive memory:

Memory update:

Transformer-XL

Memory retrieval:
Effective context:
Input segment:

Segment 1

Segment 2

Segment 1

Segment 3

Figure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL
(bottom) discards old contexts since it caches the KV states for the last segment only.

entries, it forward-passes the query vectors through the memory FFN and retrieves its
corresponding value. Like RNNs, the memory state is still bounded in MNM.
Unlike the RNNs, the attention mechanism however doesn‚Äôt maintain a recurrent state and
only performs a feed-forward computation on input sequence segment Xs :
Os = attention( Xs ).

(3)

The attention output Os is simply passed to the next layer and no state is carried over
to the next input sequence Xs+1 at the same attention layer. In the attention layer, in
order to capture the dependency between the consequent segments Xs and Xs+1 , one
needs to process them altogether at the same time and this process becomes a bottleneck
requiring large computational resources as the length of input sequence grows more and
more. To improve the efficiency while still being able to benefit from the expressiveness of
the attention mechanism, this work introduces a recurrent attention layer.

3

Method

Figure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al., 2019).
Similar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We
compute the standard causal dot-product attention context within each segment. So the
dot-product attention computation is local in a sense that it covers a total N number of
tokens of the current segment with index S (N is the segment length).
The local attention (Dai et al., 2019), however, discards the attention states of the previous
segment when processing the next one. In Infini-Transformers, instead of leaving out the
old KV attention states, we propose to reuse them to maintain the entire context history
with a compressive memory. So each attention layer of Infini-Transformers has both global
compressive and local fine-grained states. We call such an efficient attention mechanism
Infini-attention, which is illustrated in Figure 1 and described formally in the following
sections.
3.1

Infini-attention

As shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes
both local and global context states and combine them for its output. Similar to multi-head
3

Preprint. Under review.

attention (MHA), it maintains H number of parallel compressive memory per attention
layer (H is the number of attention heads) in addition to the dot-product attention and like
the RNNs and MNM, it maintains a recurrent memory state to efficiently track the long
sequence context:
Os , Ms = in f ini-attention( Xs , Ms‚àí1 )
(4)
3.1.1

Scaled Dot-product Attention

The multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention
variant (Munkhdalai et al., 2016; Cheng et al., 2016), has been the main building block in
LLMs. The MHA‚Äôs strong capability to model context-dependent dynamic computation and
its conveniences of temporal masking have been leveraged extensively in the autoregressive
generative models.
A single head in the vanilla MHA computes its attention context Adot ‚àà IR N √ódvalue from
sequence of input segments X ‚àà IR N √ódmodel as follows. First, it computes attention query,
key, and value states:
K = XWK , V = XWV and Q = XWQ .
(5)
Here, WK ‚àà IRdmodel √ódkey , WV ‚àà IRdmodel √ódvalue and WQ ‚àà IRdmodel √ódkey are trainable projection
matrices. Then, the attention context is calculated as a weighted average of all other values
as


QK T
V.
(6)
Adot = softmax ‚àö
dmodel
For MHA, we compute H number of attention context vectors for each sequence element
in parallel, concatenate them along the second dimension and then finally project the
concatenated vector to the model space to obtain the attention output.
3.1.2

Compressive Memory

In Infini-attention, instead of computing new memory entries for compressive memory, we
reuse the query, key and value states (Q, K and V) from the dot-product attention computation. The state sharing and reusing between the dot-product attention and compressive
memory not only enables efficient plug-in-play long-context adaptation but also speeds up
training and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to
store bindings of key and value states in the compressive memory and retrieve by using the
query vectors.
While there are different forms of compressive memory proposed in the literature (Hopfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and
computational efficiency, in this work we parameterize the memory with an associative
matrix (Schlag et al., 2020). This approach further allows us to cast the memory update
and retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage
stable training techniques from the related methods. Specially, we adopt the update rule
and retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and
competitive performance.
Memory retrieval. In Infini-attention, we retrieve new content Amem ‚àà IR N √ódvalue from the
memory Ms‚àí1 ‚àà IRdkey √ódvalue by using the query Q ‚àà IR N √ódkey as:
Amem =

œÉ ( Q ) Ms ‚àí 1
.
œÉ ( Q ) z s ‚àí1

(7)

Here, œÉ and zs‚àí1 ‚àà IRdkey are a nonlinear activation function and a normalization term,
respectively. As the choice of the non-linearity and the norm method is crucial for training
stability, following Katharopoulos et al. (2020) we record a sum over all keys as the normalization term zs‚àí1 and use element-wise ELU + 1 as the activation function (Clevert et al.,
2015).
4

Preprint. Under review.

Model
Transformer-XL
Compressive Transformer
Memorizing Transformers
RMT
AutoCompressors
Infini-Transformers

Memory (cache) footprint
(dkey + dvalue ) √ó H √ó N √ó l
dmodel √ó (c + N ) √ó l
(dkey + dvalue ) √ó H √ó N √ó S
dmodel √ó p √ó l √ó 2
dmodel √ó p √ó (m + 1) √ó l
dkey √ó (dvalue + 1) √ó H √ó l

Context length
N√ól
(c √ó r + N ) √ó l
N√óS
N√óS
N√óS
N√óS

Memory update
Discarded
Discarded
None
Discarded
Discarded
Incremental

Memory retrieval
Dot-product attention
Dot-product attention
kNN + dot-product attention
Soft-prompt input
Soft-prompt input
Linear attention

Table 1: Transformer models with segment-level memory are compared. For each model, the
memory size and effective context length are defined in terms of their model parameters (N:
input segment length, S: the number of segments, l: the number of layers, H: the number
of attention heads, c: Compressive Transformer memory size, r: compression ratio, p: the
number of soft-prompt summary vectors and m: summary vector accumulation steps).
Memory update. Once the retrieval is done, we update the memory and the normalization
term with the new KV entries and obtain the next states as
Ms ‚Üê Ms‚àí1 + œÉ (K ) T V and zs ‚Üê zs‚àí1 +

N

‚àë œÉ ( K t ).

(8)

t =1

The new memory states Ms and zs are then passed to the next segment S + 1, building in
a recurrence in each attention layer. The right side term œÉ (K ) T V in Eq. (8) is known as an
associative binding operator (Smolensky, 1990; Hebb, 2005; Schlag et al., 2020).
Inspired by the success of delta rule (Munkhdalai et al., 2019; Schlag et al., 2020; 2021),
we have also incorporated it into our Infini-attention. The delta rule attempts a slightly
improved memory update by first retrieving existing value entries and subtracting them
from the new values before applying the associative bindings as new update.
Ms ‚Üê Ms ‚àí 1 + œÉ ( K ) T ( V ‚àí

œÉ ( K ) Ms ‚àí 1
).
œÉ ( K ) z s ‚àí1

(9)

This update rule (Linear + Delta) leaves the associative matrix unmodified if the KV binding
already exists in the memory while still tracking the same normalization term as the former
one (Linear) for numerical stability.
Long-term context injection. We aggregate the local attention state Adot and memory
retrieved content Amem via a learned gating scalar Œ≤:
A = sigmoid( Œ≤) ‚äô Amem + (1 ‚àí sigmoid( Œ≤)) ‚äô Adot .

(10)

This adds only a single scalar value as training parameter per head while allowing a
learnable trade-off between the long-term and local information flows in the model (Wu
et al., 2022).
Similar to the standard MHA, for the multi-head Infini-attention we compute H number of
context states in parallel, and concatenate and project them for the final attention output
O ‚àà IR N √ódmodel :
O = [ A1 ; . . . A H ]WO
(11)
where WO ‚àà IR H √ódvalue √ódmodel is trainable weights.
3.2

Memory and Effective Context Window

Our Infini-Transformer enables an unbounded context window with a bounded memory
footprint. To illustrate this, Table 1 lists the previous segment-level memory models with
their context-memory footprint and effective context length defined in terms of model
parameters and input segment length. Infini-Transformer has a constant memory complexity
of dkey √ó dvalue + dkey for storing compressed context in Ms and zs for each head in single
layer while for the other models, the complexity grows along with the sequence dimension
- the memory complexity depends either on the cache size for Transformer-XL (Dai et al.,
2019), Compressive Transformer (Rae et al., 2019) and Memorizing Transformers (Wu et al.,
2022) or on the soft-prompt size for RMT (Bulatov et al., 2022) and AutoCompressors (Ge
et al., 2023).
5

Preprint. Under review.

Transformer-XL computes attention over KV
states cached from the last segment in addition
to the current states. Since this is done for each
layer, Transformer-XL extends the context window from N to N √ó l tokens with an additional
memory footprint of (dkey + dvalue ) √ó H √ó N √ó l.
Compressive Transformer adds a second cache
to Transformer-XL and stores compressed representations of past segment activations. So it
extends the Transformer-XL‚Äôs context window
by c √ó r √ó l but still has a large context-memory
complexity. Taking the idea further, Memorizing Transformers opt to store the entire KV states
as context for input sequences. Since the storage becomes prohibitively expensive in this case,
they restrict the contextual computation to a single layer only. By utilizing a fast kNN retriever,
Memorizing Transformers then build a context
window covering the entire sequence history of
length N √ó S at an increased cost of storage. Our
experiments show that Infini-Transformer LM
can achieve more than 100x compression rate on
top of Memorizing Transformers while further
improving the perplexity score.

Early
layers

Attention heads

Figure 3: There are two types of heads
emerged in Infini-attention after training: specialized heads with gating score near 0 or
1 and mixer heads with score close to 0.5.
The specialized heads either process contextual information via the local attention mechanism or retrieve from the compressive memory whereas the mixer heads aggregate both
current contextual information and long-term
memory content together into single output.

RMT and AutoCompressors allow for a potentially infinite context length since they compress
the input into summary vectors and then pass them as extra soft-prompt inputs for the subsequent segments. However, in practice the success of those techniques highly depends on the
size of soft-prompt vectors. Namely, it is necessary to increase the number of soft-prompt
(summary) vectors to achieve a better performance with AutoCompressors (Chevalier
et al., 2023) and with that, the memory and compute complexity grow quickly resulting
in diminished efficiency. It was also observed in AutoCompressors (Chevalier et al., 2023)
that an efficient compression objective is needed for training such prompt compression
techniques (Ge et al., 2023).

4

Experiments

We evaluated our Infini-Transformer models on benchmarks involving extremely long input
sequences: long-context language modeling, 1M length passkey context block retrieval
and 500K length book summarization tasks. For the language modeling benchmark, we
train our models from scratch while for the passkey and book summarization tasks, we
continually pre-train existing LLMs in order to highlight a plug-and-play long-context
adaptation capability of our approach.
4.1

Implementation details

Segment chunking. we forward-pass the entire input text a Transformer model and then
perform segment chunking at each Infini-attention layer - in this way, perform a minimal
modification to the existing Transformer implementation. The Infini-attention layer segments the input and process it segment by segment and concatenates back the segments to
pass the original-length segment as output to the next layer.
Back-propagation through time (BPTT). Each Infini-attention layer is trained with backpropagation through time (Werbos, 1988) by computing the gradient w.r.t the compressive
memory states, similar to how RNNs are trained. To save memory, we perform gradient
checkpoint when processing the sequence segment by segment.
6

Preprint. Under review.

Model
Transformer-XL
Memorizing Transformers
RMT
Infini-Transformer (Linear)
Infini-Transformer (Linear + Delta)

Memory size (comp.)
50M (3.7x)
183M (1x)
2.5M (73x)
1.6M (114x)
1.6M (114x)

XL cache
2048
2048
None
None
None

Segment length
2048
2048
2048
2048
2048

PG19
11.88
11.37
13.27
9.65
9.67

Arxiv-math
2.42
2.26
2.55
2.24
2.23

Table 2: Long-context language modeling results are compared in terms of average tokenlevel perplexity. Comp. denotes compression ratio. Infini-Transformer outperforms memorizing transformers with memory length of 65K and achieves 114x compression ratio.

Position Embeddings (PE). As shown Figure 1, we don‚Äôt use position embeddings for
the key and query vectors of the compressive memory to store only global contextual
information in the long-term memory. The PEs were applied to the QK vectors only after
the compressive memory reading and update.
4.2

Long-context Language Modeling

We trained and evaluated small Infini-Transformer models on PG19 (Rae et al., 2019) and
Arxiv-math (Wu et al., 2022) benchmarks. Our setup closely resembles that of Memorizing
Transformers (Wu et al., 2022). Namely, all our models have 12 layers and 8 attention heads
of dimension 128 each and FFNs with hidden layer 4096.
We set the Infini-attention segment length N to 2048 for all attention layers and the input
sequence length to 32768 for training. This allows the Infini-attention to unroll over 16 steps
w.r.t its compressive memory states. For the RMT baseline, we performed several runs with
summary prompt lengths 50, 100 and 150 and sequence lengths 4096, 8196 and 32768. RMT
with 100 summary vectors gave the best result when trained on 8196 length sequences.
The main results from the language modeling experiments are summarized in Table 2. Our
Infini-Transformer outperforms both Transformer-XL (Dai et al., 2019) and Memorizing
Transformers (Wu et al., 2022) baselines while maintaining 114x less memory parameters
than the Memorizing Transformer model with a vector retrieval-based KV memory with
length of 65K at its 9th layer.
100K length training. We further increased the training sequence length to 100K from
32K and trained the models on Arxiv-math dataset. 100K training further decreased the
perplexity score to 2.21 and 2.20 for Linear and Linear + Delta models.
Gating score visualization. Figure 3 visualizes the gating score, sigmoid( Œ≤) for the compressive memory for all attention heads in each layer. There are two types of heads emerged in
Infini-attention after training: specialized heads with a gating score near 0 or 1 and mixer
heads with a score close to 0.5. The specialized heads either process contextual information
via the local attention computation or retrieve from the compressive memory whereas the
mixer heads aggregate both current contextual information and long-term memory content
together into a single output. Interestingly, each layer has at least a single short-range
head, allowing a forward-propagation of input signal up until the output layer. We also

Zero-shot
Infini-Transformer (Linear)
Infini-Transformer (Linear + Delta)

32K
14/13/98
13/11/99

128K
11/14/100
6/9/99

Infini-Transformer (Linear)
Infini-Transformer (Linear + Delta)

100/100/100
100/100/100

100/100/100
100/100/99

256K
6/3/100
7/5/99

512K
6/7/99
6/8/97

1M
8/6/98
7/6/97

97/99/100
100/100/100

96/94/100
100/100/100

FT (400 steps)
100/100/100
100/100/99

Table 3: Infini-Transformers solved the passkey task with up to 1M context length when
fine-tuned on 5K length inputs. We report token-level retrieval accuracy for passkeys hidden
in a different part (start/middle/end) of long inputs with lengths 32K to 1M.
7

Preprint. Under review.

Model
BART
BART + Unlimiformer
PRIMERA
PRIMERA + Unlimiformer
Infini-Transformers (Linear)
Infini-Transformers (Linear + Delta)

Rouge-1
36.4
36.8
38.6
37.9
37.9
40.0

Rouge-2
7.6
8.3
7.2
8.2
8.7
8.8

Rouge-L
15.3
15.7
15.6
16.3
17.6
17.9

Overall
16.2
16.9
16.3
17.2
18.0
18.5

Table 4: 500K length book summarization (BookSum) results. The BART, PRIMERA and
Unlimiformer results are from Bertsch et al. (2024).
observed an interleaving of long and short-term content retrievals throughout the forward
computation.
4.3

LLM Continual Pre-training

We performed a lightweight continual pre-training for long-context adaptation of existing
LLMs. The pre-training data includes the PG19 and Arxiv-math corpus as well as C4
text (Raffel et al., 2020) with length more than 4K tokens. The segment length N was set to
2K throughout our experiments.
1M passkey retrieval benchmark. We replaced the vanilla MHA in a 1B LLM with Infiniattention and continued to pre-train on inputs with length of 4K. The model was trained for
30K steps with batch size of 64 before fine-tuning on the passkey retrieval task (Mohtashami
& Jaggi, 2024).
The passkey task hides a random number into a long text and asks it back at the model
output. The length of the distraction text is varied by repeating a text chunk multiple times.
The previous work (Chen et al., 2023a) showed that a 8B LLaMA model can solve the task up
to 32K length when fine-tuned with the same 32K length inputs with Position Interpolation.
We take this challenge further and fine-tune on only 5K length inputs to test on 1M length
regime.
Table 3 reports the token-level accuracy for
test subsets with input lengths ranging from
32K to 1M. For each test subset, we controlled the position of the passkey so that it
is either located around the beginning, middle or the end of the input sequence. We
reported both zero-shot accuracy and finetuning accuracy. Infini-Transformers solved
the task with up to 1M context length after fine-tuning on 5K length inputs for 400
steps.

Rouge overall score

20

19

18

17
16K

32K

64K

128K

256K

500K

Input length

500K length book summarization (BookSum). We further scaled our approach by
Figure 4: Infini-Transformers obtain better
continuously pre-training a 8B LLM model
Rouge overall scores with more book text prowith 8K input length for 30K steps. We then
vided as input.
fine-tuned on a book summarization task,
BookSum (KrysÃÅcinÃÅski et al., 2021) where the
goal is to generate a summary of an entire
book text.
We set the input length to 32K for fine-tuning and increase to 500K for evaluating. We use a
generation temperature of 0.5 and top p = 0.95 and set the number of decoding steps to 1024
to generate a summary of each book.
Table 4 compares our model against the encoder-decoder models that were built particularly
for the summarization task (Lewis et al., 2019; Xiao et al., 2021) and their retrieval-based
long-context extension (Bertsch et al., 2024). Our model outperforms the previous best
8

Preprint. Under review.

results and achieves a new SOTA on BookSum by processing the entire text from book. We
have also plotted the overall Rouge score on validation split of BookSum data in Figure 4.
There is a clear trend showing that with more text provided as input from books, Our
Infini-Transformers improves its summarization performance metric.

5

Related Work

Compressive memory. Inspired by the plasticity in biological neurons (Munkhdalai & Yu,
2017a; Miconi et al., 2018), compressive memory approaches cast parameterized functions
as memory to store and retrieve information (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba
et al., 2016; Munkhdalai et al., 2019). Unlike the Transformer KV memory array (Vaswani
et al., 2017; Wu et al., 2022), which grows with input sequence length, compressive memory
systems maintain a constant number of memory parameters for computational efficiency.
The parameters are modified with an update rule to store information, which is then
retrieved via a memory reading mechanism (Graves et al., 2014; Sukhbaatar et al., 2015;
Munkhdalai & Yu, 2017b).
Compressed input representations can be viewed as a summary of past sequence segments (Rae et al., 2019; Chevalier et al., 2023). Along this direction, more recent works
have been utilizing a Transformer LLM itself to compress input sequence for efficient longcontext modeling (Bulatov et al., 2022; Chevalier et al., 2023; Ge et al., 2023; Mu et al., 2024;
Hwang et al., 2024). However, the previous segment-level compression methods, including
Compressive Transformers (Rae et al., 2019) still discard the memory entries of old segments
in order to free up space for the new ones, limiting their context window to the most recent
segments. This is in contrast to our Infini-attention that computes incremental memory
updates to a fixed amount of memory parameters in a recurrent fashion.
Long-context continual pre-training. There is a line of work that extends the dot-product
attention layers and continues to train LLMs for long-context (Xiong et al., 2023; Fu et al.,
2024). The attention extensions include incorporating sparsity into the attention layer (Chen
et al., 2023b; Ratner et al., 2022; Mohtashami & Jaggi, 2024) as well as manipulating the
position encodings (Chen et al., 2023a; Peng et al., 2023). Although the position encodingbased methods such as position interpolation techniques (Chen et al., 2023a) can be data
efficient as they only adjust the positional bias in the attention layer, they are still costly for
inference.
The attention mechanism is also prone to the issues of attention sink (Xiao et al., 2023) and
lost-in-the-middle (Liu et al., 2024). Consequently, they struggle in a regime where context
length is longer than what was observed during training (Press et al., 2021; Kazemnejad
et al., 2024). The proposed Infini-attention addresses those issues by enabling a segmentlevel streaming computation over long sequences with a fixed local attention window. Our
Infini-Transformers successfully extrapolate to 1M input length regimes when trained on
32K and even 5K length sequences.
Efficient attention. The efficient attention techniques attempt to improve the efficiency of
the dot-product attention with an approximation or a system-level optimization. Multiple
directions have been explored for different forms of efficient attention approximation,
including sparsity-based (Child et al., 2019; Beltagy et al., 2020; Sukhbaatar et al., 2021;
Ding et al., 2023; Xiao et al., 2024) and linear attention approximation (Shen et al., 2018;
Katharopoulos et al., 2020; Schlag et al., 2021). Among those, the linear attention variants
are closely related to the associative memory matrix (Schlag et al., 2020; 2021) and the
metalearned neural memory (Munkhdalai et al., 2019), where KV bindings (Smolensky,
1990) are stored in Fast-Weights (Hinton & Plaut, 1987; Schmidhuber, 1992; Ba et al., 2016)
that are modified in with respect to new contextual information. More recently, system-level
optimization techniques have been proposed by leveraging specific hardware architecture
to make the exact attention computation more efficient (Dao et al., 2022; Liu et al., 2023).
9

Preprint. Under review.

6

Conclusion

An effective memory system is crucial not just for comprehending long contexts with LLMs,
but also for reasoning, planning, continual adaptation for fresh knowledge, and even for
learning how to learn. This work introduces a close integration of compressive memory module into the vanilla dot-product attention layer. This subtle but critical modification to the
attention layer enables LLMs to process infinitely long contexts with bounded memory and
computation resources. We show that our approach can naturally scale to a million length
regime of input sequences, while outperforming the baselines on long-context language
modeling benchmark and book summarization tasks. We also demonstrate a promising
length generalization capability of our approach. 1B model that was fine-tuned on up to 5K
sequence length passkey instances solved the 1M length problem.
Acknowledgments
We would like to thank Dongseong Hwang for their help implementing efficient sequence
unrolling mechanism with the jax scan function. We would also like to thank Aditya Gupta,
Kalpesh Krishna, Tu Vu and Alexandra Chronopoulou for their feedback.

References
Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre
Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2
technical report. arXiv preprint arXiv:2305.10403, 2023.
Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using
fast weights to attend to the recent past. Advances in neural information processing systems,
29, 2016.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020.
Amanda Bertsch, Uri Alon, Graham Neubig, and Matthew Gormley. Unlimiformer: Longrange transformers with unlimited length input. Advances in Neural Information Processing
Systems, 36, 2024.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. Advances in neural information processing systems,
33:1877‚Äì1901, 2020.
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances
in Neural Information Processing Systems, 35:11079‚Äì11091, 2022.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint
arXiv:2306.15595, 2023a.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia.
Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint
arXiv:2309.12307, 2023b.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for
machine reading. arXiv preprint arXiv:1601.06733, 2016.
Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language
models to compress contexts. arXiv preprint arXiv:2305.14788, 2023.
10

Preprint. Under review.

Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with
sparse transformers. arXiv preprint arXiv:1904.10509, 2019.
Djork-ArneÃÅ Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep
network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv
preprint arXiv:1901.02860, 2019.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher ReÃÅ. Flashattention: Fast
and memory-efficient exact attention with io-awareness. Advances in Neural Information
Processing Systems, 35:16344‚Äì16359, 2022.
Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang,
Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens.
arXiv preprint arXiv:2307.02486, 2023.
Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and
Hao Peng. Data engineering for scaling language models to 128k context. arXiv preprint
arXiv:2402.10171, 2024.
Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context
compression in a large language model. arXiv preprint arXiv:2307.06945, 2023.
Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint
arXiv:1410.5401, 2014.
Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,
Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024.
Donald Olding Hebb. The organization of behavior: A neuropsychological theory. Psychology
press, 2005.
Geoffrey E Hinton and David C Plaut. Using fast weights to deblur old memories. In
Proceedings of the ninth annual conference of the Cognitive Science Society, pp. 177‚Äì186, 1987.
Sepp Hochreiter and JuÃàrgen Schmidhuber. Long short-term memory. Neural computation, 9
(8):1735‚Äì1780, 1997.
John J Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554‚Äì2558, 1982.
Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, Khe Chai Sim, and Pedro Moreno
Mengibar. Transformerfam: Feedback attention is working memory. arXiv preprint
arXiv:2404.09173, 2024.
≈Åukasz Kaiser and Ilya Sutskever.
arXiv:1511.08228, 2015.

Neural gpus learn algorithms.

arXiv preprint

Pentti Kanerva. Sparse distributed memory. MIT press, 1988.
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FrancÃßois Fleuret. Transformers
are rnns: Fast autoregressive transformers with linear attention. In International conference
on machine learning, pp. 5156‚Äì5165. PMLR, 2020.
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and
Siva Reddy. The impact of positional encoding on length generalization in transformers.
Advances in Neural Information Processing Systems, 36, 2024.
Wojciech KrysÃÅcinÃÅski, Nazneen Rajani, Divyansh Agarwal, Caiming Xiong, and Dragomir
Radev. Booksum: A collection of datasets for long-form narrative summarization. arXiv
preprint arXiv:2105.08209, 2021.
11

Preprint. Under review.

Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. arXiv
preprint arXiv:1910.13461, 2019.
Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for
near-infinite context. arXiv preprint arXiv:2310.01889, 2023.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language models use long contexts. Transactions
of the Association for Computational Linguistics, 12:157‚Äì173, 2024.
Wolfgang Maass, Thomas NatschlaÃàger, and Henry Markram. Real-time computing without
stable states: A new framework for neural computation based on perturbations. Neural
computation, 14(11):2531‚Äì2560, 2002.
Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic
neural networks with backpropagation. In International Conference on Machine Learning,
pp. 3559‚Äì3568. PMLR, 2018.
Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for
transformers. Advances in Neural Information Processing Systems, 36, 2024.
Jesse Mu, Xiang Li, and Noah Goodman. Learning to compress prompts with gist tokens.
Advances in Neural Information Processing Systems, 36, 2024.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International conference on machine
learning, pp. 2554‚Äì2563. PMLR, 2017a.
Tsendsuren Munkhdalai and Hong Yu. Neural semantic encoders. In Proceedings of the
conference. Association for Computational Linguistics. Meeting, volume 1, pp. 397. NIH Public
Access, 2017b.
Tsendsuren Munkhdalai, John P Lalor, and Hong Yu. Citation analysis with neural attention
models. In Proceedings of the Seventh International Workshop on Health Text Mining and
Information Analysis, pp. 69‚Äì77, 2016.
Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischler. Metalearned
neural memory. Advances in Neural Information Processing Systems, 32, 2019.
Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context
window extension of large language models. arXiv preprint arXiv:2309.00071, 2023.
Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury,
Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems, 5, 2023.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear
biases enables input length extrapolation. arXiv preprint arXiv:2108.12409, 2021.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a
unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485‚Äì5551,
2020.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve
in-context learning of large language models. arXiv preprint arXiv:2212.10947, 2022.
Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, JuÃàrgen Schmidhuber,
and Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math
problem solving. arXiv preprint arXiv:1910.06611, 2019.
12

Preprint. Under review.

Imanol Schlag, Tsendsuren Munkhdalai, and JuÃàrgen Schmidhuber. Learning associative
inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020.
Imanol Schlag, Kazuki Irie, and JuÃàrgen Schmidhuber. Linear transformers are secretly
fast weight programmers. In International Conference on Machine Learning, pp. 9355‚Äì9366.
PMLR, 2021.
JuÃàrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic
recurrent networks. Neural Computation, 4(1):131‚Äì139, 1992.
Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear
memory cost. In International Conference on Machine Learning, pp. 4596‚Äì4604. PMLR, 2018.
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient
attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018.
Paul Smolensky. Tensor product variable binding and the representation of symbolic
structures in connectionist systems. Artificial intelligence, 46(1-2):159‚Äì216, 1990.
Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks.
Advances in neural information processing systems, 28, 2015.
Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston,
and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In
International Conference on Machine Learning, pp. 9902‚Äì9912. PMLR, 2021.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,
Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:
Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market
model. Neural networks, 1(4):339‚Äì356, 1988.
Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing
transformers. arXiv preprint arXiv:2203.08913, 2022.
Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang,
Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity
of llms for understanding extremely long sequences with training-free memory. arXiv
preprint arXiv:2402.04617, 2024.
Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.
Wen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman Cohan. Primera: Pyramidbased masked sentence pre-training for multi-document summarization. arXiv preprint
arXiv:2110.08499, 2021.
Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis
Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, et al. Effective
long-context scaling of foundation models. arXiv preprint arXiv:2309.16039, 2023.

A

Additional Training Details

For the long-context language modeling task, we set the learning rate to 0.01 by performing small search over values of 0.003, 0.005, 0.01 and 0.03. We used the Adafactor optimizer (Shazeer & Stern, 2018) with linear warmup with 1000 steps, followed by cosine
decay. We applied gradient checkpointing after each segment to save to save memory. The
batch size was set to 64. For the LLM experiments, we set the learning rate to 0.0001 during
continual pre-training and task fine-tuning.
13

Preprint. Under review.

B

Passkey Retrieval Task

Below we showed the input format of the passkey task.
There is an important info hidden inside a lot of irrelevant text. Find it and memorize them. I
will quiz you about the important information there. The grass is green. The sky is blue. The sun
is yellow. Here we go. There and back again. (repeat x times) The pass key is 9054. Remember
it. 9054 is the pass key. The grass is green. The sky is blue. The sun is yellow. Here we go.
There and ack again. (repeat y times) What is the pass key? The pass key is

14




Self-Supervised Learning from Images with a
Joint-Embedding Predictive Architecture

1

Meta AI (FAIR)

2

McGill University

3

Mila, Quebec AI Institute

82

This paper demonstrates an approach for learning
highly semantic image representations without relying on
hand-crafted data-augmentations. We introduce the Imagebased Joint-Embedding Predictive Architecture (I-JEPA), a
non-generative approach for self-supervised learning from
images. The idea behind I-JEPA is simple: from a single
context block, predict the representations of various target
blocks in the same image. A core design choice to guide
I-JEPA towards producing semantic representations is the
masking strategy; specifically, it is crucial to (a) sample target blocks with sufficiently large scale (semantic), and to (b)
use a sufficiently informative (spatially distributed) context
block. Empirically, when combined with Vision Transformers, we find I-JEPA to be highly scalable. For instance, we
train a ViT-Huge/14 on ImageNet using 16 A100 GPUs in
under 72 hours to achieve strong downstream performance
across a wide range of tasks, from linear classification to
object counting and depth prediction.

1. Introduction
In computer vision, there are two common families
of approaches for self-supervised learning from images:
invariance-based methods [1, 4, 10, 17, 18, 24, 35, 37, 74] and
generative methods [8, 28, 36, 57].
Invariance-based pretraining methods optimize an encoder to produce similar embeddings for two or more views
of the same image [15, 20], with image views typically
constructed using a set of hand-crafted data augmentations,
such as random scaling, cropping, and color jittering [20],
amongst others [35]. These pretraining methods can produce representations of a high semantic level [4, 18], but
they also introduce strong biases that may be detrimental
for certain downstream tasks or even for pretraining tasks
with different data distributions [2]. Often, it is unclear
* massran@meta.com

4

New York University

ImageNet-1K Linear Evaluation vs GPU Hours

Abstract

I-JEPA

ViT-H/16448 (300ep)

81

Top 1 (%)

arXiv:2301.08243v3 [cs.CV] 13 Apr 2023

Mahmoud Assran1,2,3 * Quentin Duval1 Ishan Misra1 Piotr Bojanowski1
Pascal Vincent1 Michael Rabbat1,3 Yann LeCun1,4 Nicolas Ballas1

80

I-JEPA

ViT-H/14 (300ep)

79
CAE

78

ViT-L/16 (1600ep)

iBOT

ViT-S/16 (800ep)

77
76

data2vec

MAE

ViT-H/14 (1600ep)

ViT-L/16 (1600ep)

103

104

Pretraining GPU Hours

Figure 1. ImageNet Linear Evaluation. The I-JEPA method
learns semantic image representations without using any view data
augmentations during pretraining. By predicting in representation
space, I-JEPA produces semantic representations while using less
compute than previous methods.

how to generalize these biases for tasks requiring different levels of abstraction. For example, image classification
and instance segmentation do not require the same invariances [11]. Additionally, it is not straightforward to generalize these image-specific augmentations to other modalities such as audio.
Cognitive learning theories have suggested that a driving mechanism behind representation learning in biological systems is the adaptation of an internal model to predict sensory input responses [31, 59]. This idea is at the
core of self-supervised generative methods, which remove
or corrupt portions of the input and learn to predict the corrupted content [9, 36, 57, 67, 68, 71]. In particular, maskdenoising approaches learn representations by reconstructing randomly masked patches from an input, either at the
pixel or token level. Masked pretraining tasks require less
prior knowledge than view-invariance approaches and easily generalize beyond the image modality [8]. However, the

sx

D (s x , sy )

z

decoder

sy
x-encoder

y-encoder

x-encoder

x

y

x

(a) Joint-Embedding Architecture

yÃÇ

D (yÃÇ, y)

y

(b) Generative Architecture

z

predictor

sÃÇy

D (sÃÇy , sy )
sy

x-encoder

y-encoder

x

y

(c) Joint-Embedding Predictive Architecture

Figure 2. Common architectures for self-supervised learning, in which the system learns to capture the relationships between its inputs.
The objective is to assign a high energy (large scaler value) to incompatible inputs, and to assign a low energy (low scaler value) to compatible inputs. (a) Joint-Embedding Architectures learn to output similar embeddings for compatible inputs x, y and dissimilar embeddings
for incompatible inputs. (b) Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a decoder
network that is conditioned on additional (possibly latent) variables z to facilitate reconstruction. (c) Joint-Embedding Predictive Architectures learn to predict the embeddings of a signal y from a compatible signal x, using a predictor network that is conditioned on additional
(possibly latent) variables z to facilitate prediction.

resulting representations are typically of a lower semantic
level and underperform invariance-based pretraining in offthe-shelf evaluations (e.g., linear-probing) and in transfer
settings with limited supervision for semantic classification
tasks [4]. Consequently, a more involved adaptation mechanism (e.g., end-to-end fine-tuning) is required to reap the
full advantage of these methods.
In this work, we explore how to improve the semantic
level of self-supervised representations without using extra
prior knowledge encoded through image transformations.
To that end, we introduce a joint-embedding predictive architecture [48] for images (I-JEPA). An illustration of the
method is provided in Figure 3. The idea behind I-JEPA
is to predict missing information in an abstract representation space; e.g., given a single context block, predict the
representations of various target blocks in the same image, where target representations are computed by a learned
target-encoder network.
Compared to generative methods that predict in
pixel/token space, I-JEPA makes use of abstract prediction
targets for which unnecessary pixel-level details are potentially eliminated, thereby leading the model to learn more
semantic features. Another core design choice to guide
I-JEPA towards producing semantic representations is the
proposed multi-block masking strategy. Specifically, we
demonstrate the importance of predicting sufficiently large
target blocks in the image, using an informative (spatially
distributed) context block.
Through an extensive empirical evaluation, we demonstrate that:
‚Ä¢ I-JEPA learns strong off-the-shelf representations
without the use of hand-crafted view augmentations
(cf. Fig.1). I-JEPA outperforms pixel-reconstruction
methods such as MAE [36] on ImageNet-1K linear
probing, semi-supervised 1% ImageNet-1K, and semantic transfer tasks.
‚Ä¢ I-JEPA is competitive with view-invariant pretraining

approaches on semantic tasks and achieves better performance on low-level visions tasks such as object
counting and depth prediction (Sections 5 and 6). By
using a simpler model with less rigid inductive bias,
I-JEPA is applicable to a wider set of tasks.
‚Ä¢ I-JEPA is also scalable and efficient (Section 7). Pretraining a ViT-H/14 on ImageNet requires less than
1200 GPU hours, which is over 2.5√ó faster than a ViTS/16 pretrained with iBOT [79] and over 10√ó more efficient than a ViT-H/14 pretrained with MAE. Predicting in representation space significantly reduces the total computation needed for self-supervised pretraining.

2. Background
Self-supervised learning is an approach to representation learning in which a system learns to capture the relationships between its inputs. This objective can be readily described using the framework of Energy-Based Models
(EBMs) [49] in which the self-supervised objective is to assign a high energy to incompatible inputs, and to assign a
low energy to compatible inputs. Many existing generative
and non-generative approaches to self-supervised learning
can indeed be cast in this framework; see Figure 2.
Joint-Embedding Architectures. Invariance-based pretraining can be cast in the framework of EBMs using a
Joint-Embedding Architecture (JEA), which learns to output similar embeddings for compatible inputs, x, y, and dissimilar embeddings for incompatible inputs; see Figure 2a.
In the context of image-based pretraining, compatible x, y
pairs are typically constructed by randomly applying handcrafted data augmentations to the same input image [20].
The main challenge with JEAs is representation collapse,
wherein the energy landscape is flat (i.e., the encoder produces a constant output regardless of the input). During
the past few years, several approaches have been investi-

gated to prevent representation collapse, such as contrastive
losses that explicitly push apart embeddings of negative examples [15,24,37], non-contrastive losses that minimize the
informational redundancy across embeddings [10, 74], and
clustering-based approaches that maximize the entropy of
the average embedding [4, 5, 18]. There are also heuristic
approaches that leverage an asymmetric architectural design between the x-encoder and y-encoder to avoid collapse [8, 24, 35].
Generative Architectures. Reconstruction-based methods for self-supervised learning can also be cast in the
framework of EBMs using Generative Architectures; see
Figure 2b. Generative Architectures learn to directly reconstruct a signal y from a compatible signal x, using a
decoder network that is conditioned on an additional (possibly latent) variable z to facilitate reconstruction. In the
context of image-based pretraining, one common approach
in computer vision is to produce compatible x, y pairs using
masking [9, 38] where x is a copy of the image y, but with
some of the patches masked. The conditioning variable z
then corresponds to a set of (possibly learnable) mask and
position tokens, that specifies to the decoder which image
patches to reconstruct. Representation collapse is not a concern with these architectures as long as the informational
capacity of z is low compared to the signal y.
Joint-Embedding Predictive Architectures. As shown
in Figure 2c, Joint-Embedding Predictive Architectures [48]
are conceptually similar to Generative Architectures; however, a key difference is that the loss function is applied in
embedding space, not input space. JEPAs learn to predict
the embeddings of a signal y from a compatible signal x,
using a predictor network that is conditioned on an additional (possibly latent) variable z to facilitate prediction.
Our proposed I-JEPA provides an instantiation of this architecture in the context of images using masking; see Figure 3.
In contrast to Joint-Embedding Architectures, JEPAs do
not seek representations invariant to a set of hand-crafted
data augmentations, but instead seek representations that
are predictive of each other when conditioned on additional information z. However, as with Joint-Embedding
Architectures, representation collapse is also a concern with
JEPAs; we leverage an asymmetric architecture between the
x- and y-encoders to avoid representation collapse.

3. Method
We now describe the proposed Image-based JointEmbedding Predictive Architecture (I-JEPA), illustrated in
Figure 3. The overall objective is as follows: given a context
block, predict the representations of various target blocks

predictor
context
encoder

gœÜ

context

fŒ∏

gœÜ

gœÜ

target
encoder
target

f Œ∏ÃÑ

L2

Figure 3. I-JEPA. The Image-based Joint-Embedding Predictive
Architecture uses a single context block to predict the representations of various target blocks originating from the same image.
The context encoder is a Vision Transformer (ViT), which only
processes the visible context patches. The predictor is a narrow
ViT that takes the context encoder output and, conditioned on positional tokens (shown in color), predicts the representations of a
target block at a specific location. The target representations correspond to the outputs of the target-encoder, the weights of which
are updated at each iteration via an exponential moving average of
the context encoder weights.

in the same image. We use a Vision Transformer [29, 63]
(ViT) architecture for the context-encoder, target-encoder,
and predictor. A ViT is composed of a stack of transformer
layers, each consisting of a self-attention [66] operation followed by a fully-connected MLP. Our encoder/predictor architecture is reminiscent of the generative masked autoencoders (MAE) [36] method. However, one key difference
is that the I-JEPA method is non-generative and the predictions are made in representation space.
Targets. We first describe how we produce the targets in
the I-JEPA framework: in I-JEPA, the targets correspond
to the representations of image blocks. Given an input image y, we convert it into a sequence of N non-overlapping
patches, and feed this through the target-encoder fŒ∏ÃÑ to
obtain a corresponding patch-level representation sy =
{sy1 , . . . , syN } where syk is the representation associated
with the k th patch. To obtain the targets for our loss, we
randomly sample M (possibly overlapping) blocks from the
target representations sy . We denote by Bi the mask corresponding of the ith block and by sy (i) = {syj }j‚ààBi its

original

context

targets

parameterized by a shared learnable vector with an added
positional embedding. Since we wish to make predictions
for M target blocks, we apply our predictor M times, each
time conditioning on the mask tokens corresponding to the
target-block locations we wish to predict, and obtain predictions sÃÇy (1), . . . , sÃÇy (M ).
Loss. The loss is simply the average L2 distance between
the predicted patch-level representations sÃÇy (i) and the target patch-level representation sy (i); i.e.,
  \frac {1}{M} \sum ^M_{i=1} {D}\left (\hat {\vs }_y(i), \vs _y(i)\right ) = \frac {1}{M} \sum ^M_{i=1}\sum _{j \in B_i} \lVert \hat {\vs }_{y_j} - \vs _{y_j}\rVert ^2_2. 

Figure 4. Examples of our context and target-masking strategy.
Given an image, we randomly sample 4 target blocks with scale
in the range (0.15, 0.2) and aspect ratio in the range (0.75, 1.5).
Next, we randomly sample a context block with scale in the range
(0.85, 1.0) and remove any overlapping target blocks. Under this
strategy, the target-blocks are relatively semantic, and the contextblock is informative, yet sparse (efficient to process).

The parameters of the predictor, œï, and context encoder, Œ∏,
are learned through gradient-based optimization, while the
parameters of the target encoder Œ∏ÃÑ are updated via an exponential moving average of the context-encoder parameters.
The use of an exponential moving average target-encoder
has proven essential for training JEAs with Vision Transformers [18, 25, 79], we find the same to be true for I-JEPA.

4. Related Work
patch-level representation. Typically, we set M equal to
4, and sample the blocks with a random aspect ratio in the
range (0.75, 1.5) and random scale in the range (0.15, 0.2).
Note that the target blocks are obtained by masking the output of the target-encoder, not the input. This distinction is
crucial to ensure target representations of a high semantic
level; see, e.g., [8].
Context. Recall, the goal behind I-JEPA is to predict the
target block representations from a single context block.
To obtain the context in I-JEPA, we first sample a single
block x from the image with a random scale in the range
(0.85, 1.0) and unit aspect ratio. We denote by Bx the mask
associated with the context block x. Since the target blocks
are sampled independently from the context block, there
may be significant overlap. To ensure a non-trivial prediction task, we remove any overlapping regions from the context block. Figure 4 shows examples of various context and
target blocks in practice. Next, the masked context block,
x, is fed through the context encoder fŒ∏ to obtain a corresponding patch-level representation sx = {sxj }j‚ààBx .
Prediction. Given the output of the context encoder, sx ,
we wish to predict the M target block representations
sy (1), . . . , sy (M ). To that end, for a given target block
sy (i) corresponding to a target mask Bi , the predictor
gœï (¬∑, ¬∑) takes as input the output of the context encoder
sx and a mask token for each patch we wish to predict,
{mj }j‚ààBi , and outputs a patch-level prediction sÃÇy (i) =
{sÃÇyj }j‚ààBi = gœï (sx , {mj }j‚ààBi ). The mask tokens are

A long line of work has explored visual representation
learning by predicting the values of missing or corrupted
sensory inputs. Denoising autoencoders use random noise
as input corruption [67]. Context encoders regress an entire
image region based on its surrounding [57]. Other works
cast image colorization as a denoising task [46, 47, 77].
The idea of image denoising has recently been revisited in the context of masked image modelling [9, 36, 71],
where a Vision Transformer [29] is used to reconstruct
missing input patches. The work on Masked Autoencoders (MAE) [36] proposed an efficient architecture that
only requires the encoder to process visible image patches.
By reconstructing missing patches in pixels space, MAE
achieves strong performance when fine-tuned end-to-end on
large labeled datasets and exhibits good scaling properties.
BEiT [9] predicts the value of missing patches in a tokenized space; specifically, tokenizing image patches using
a frozen discreteVAE, which is trained on a dataset containing 250 million images [58]. Yet, pixel-level pre-training
has been shown to outperform BEiT for fine-tuning [36].
Another work, SimMIM [71], explores reconstruction targets based on the classic Histogram of Gradients [27] feature space, and demonstrates some advantage over pixel
space reconstruction. Different from those works, our representation space is learned during training through a JointEmbedding Predictive Architecture. Our goal is to learn
semantic representations that do not require extensive finetuning on downstream tasks.
Closest to our work is data2vec [8] and Context Autoencoders [25]. The data2vec method learns to predict the rep-

Method

Top-1

Method

Methods without view data augmentations
data2vec [8]
ViT-L/16
1600

Arch.

Epochs

77.3

Methods without view data augmentations
data2vec [8]
ViT-L/16
1600

73.3

MAE [36]

ViT-B/16
ViT-L/16
ViT-H/14

1600
1600
1600

68.0
76.0
77.2

CAE [22]

ViT-B/16
ViT-L/16

1600
1600

ViT-B/16
ViT-L/16
ViT-H/14
ViT-H/16448

600
600
300
300

I-JEPA

Arch.

Epochs

Top-1

MAE [36]

ViT-L/16
ViT-H/14

1600
1600

67.1
71.5

70.4
78.1

I-JEPA

ViT-L/16
ViT-H/14
ViT-H/16448

600
300
300

69.4
73.3
77.3

72.9
77.5
79.3
81.1

Methods using extra view data augmentations
iBOT [79]
ViT-B/16
400
DINO [18]
ViT-B/8
300
SimCLR v2 [35] RN151 (2√ó) 800
BYOL [35]
RN200 (2√ó) 800
MSN [4]
ViT-B/4
300

Methods using extra view data augmentations
SimCLR v2 [21] RN152 (2√ó) 800
79.1
DINO [18]
ViT-B/8
300
80.1
iBOT [79]
ViT-L/16
250
81.0
Table 1. ImageNet. Linear-evaluation on ImageNet-1k (the ViTH/16448 is pretrained at at a resolution of 448 √ó 448). I-JEPA improves linear probing performance compared to other methods that
do not rely on hand-crafted view data-augmentations during pretraining. Moreover, I-JEPA demonstrates good scalability ‚Äî the
larger I-JEPA model matches the performance of view-invariance
approaches without requiring view data-augmentations.

69.7
70.0
70.2
71.2
75.7

Table 2.
ImageNet-1%.
Semi-supervised evaluation on
ImageNet-1K using only 1% of the available labels. Models are
adapted via fine-tuning or linear-probing, depending on whichever
works best for each respective method. ViT-H/16448 is pretrained
at at a resolution of 448 √ó 448. I-JEPA pretraining outperforms
MAE which also does not rely on hand-crafted data-augmentations
during pretraining. Moreover, I-JEPA benefits from scale. A ViTH/16 trained at resolution 448 surpasses previous methods including methods that leverage extra hand-crafted data-augmentations.

5. Image Classification
resentation of missing patches computed through an online
target encoder; by avoiding handcrafted augmentations, the
method can be applied to diverse modalities with promising
results in vision, text and speech. Context Autoencoders
use an encoder/decoder architecture optimized via the sum
of a reconstruction loss and an alignment constraint, which
enforces predictability of missing patches in representation
space. Compared to these methods, I-JEPA exhibits significant improvements in computational efficiency and learns
more semantic off-the-shelf representations. Concurrent to
our work, data2vec-v2 [7] explores efficient architectures
for learning with various modalities.
We also compare I-JEPA with various methods based on
joint-embedding architectures; e.g., DINO [18], MSN [4]
and iBOT [79]. Theses methods rely on hand-crafted data
augmentations during pretraining to learn semantic image
representations. The work on MSN [4], uses masking as
an additional data-augmentation during pretraining, while
iBOT combines a data2vec-style patch-level reconstruction loss with the DINO view-invariance loss. Common
to these approaches is the need to process multiple usergenerated views of each input image, thereby hindering
scalability. By contrast, I-JEPA only requires processing
a single view of each image. We find that a ViT-Huge/14
trained with I-JEPA requires less computational effort than
a ViT-Small/16 trained with iBOT.

To demonstrate that I-JEPA learns high-level representations without relying on hand-crafted data-augmentations,
we report results on various image classification tasks using the linear probing and partial fine-tuning protocols. In
this section, we consider self-supervised models that have
been pretrained on the ImageNet-1K dataset [60]. Pretraining and evaluation implementation details are described in
the Appendix A. All I-JEPA models are trained at resolution
224 √ó 224 pixels, unless stated otherwise.
ImageNet-1K. Table 1 shows performance on the common ImageNet-1K linear-evaluation benchmark. After selfsupervised pretraining, the model weights are frozen and a
linear classifier is trained on top using the full ImageNet1K training set. Compared to popular methods such as
Masked Autoencoders (MAE) [36], Context Autoencoders
(CAE) [22], and data2vec [8], which also do not rely on extensive hand-crafted data-augmentations during pretraining,
we see that I-JEPA significantly improves linear probing
performance, while using less computational effort (see section 7). By leveraging the improved efficiency of I-JEPA,
we can train larger models that outperform the best CAE
model while using a fraction of the compute. I-JEPA also
benefits from scale; in particular, a ViT-H/16 trained at resolution 448 √ó 448 pixels matches the performance of view-

Method

Arch.

CIFAR100

Methods without view data augmentations
data2vec [8] ViT-L/16
81.6
MAE [36]
ViT-H/14
77.3
ViT-H/14
87.5
I-JEPA

Places205

iNat18

Method

Arch.

Clevr/Count

54.6
55.0
58.4

28.1
32.9
47.6

Methods without view data augmentations
data2vec [8] ViT-L/16
85.3
MAE [36]
ViT-H/14
90.5
ViT-H/14
86.7
I-JEPA

Methods using extra view data augmentations
DINO [18]
ViT-B/8
84.9
57.9
iBOT [79]
ViT-L/16
88.3
60.4

55.9
57.3

Methods using extra data augmentations
DINO [18]
ViT-B/8
86.6
iBOT [79]
ViT-L/16
85.7

Clevr/Dist

71.3
72.4
72.4
53.4
62.8

Table 3. Linear-probe transfer for image classification. Linearevaluation on downstream image classification tasks. I-JEPA significantly outperforms previous methods that also do not use augmentations (MAE and data2vec), and decreases the gap with the
best view-invariance-based methods that leverage hand-crafted
data augmentations during pretraining.

Table 4. Linear-probe transfer for low-level tasks. Linearevaluation on downstream low-level tasks consisting of object
counting (Clevr/Count) and depth prediction (Clevr/Dist). The IJEPA method effectively captures low-level image features during pretraining and outperforms view-invariance based methods
on tasks such object counting and depth prediction.

invariant approaches such as iBOT [79], despite avoiding
the use of hand-crafted data-augmentations.

view-invariance based methods that leverage extra handcrafted data augmentations. In this section, we find that
I-JEPA also learns local image features and surpasses viewinvariance based methods on low-level and dense prediction
tasks, such as object counting and depth prediction.
Table 4 shows performance on various low-level tasks
using a linear probe. After pretraining, the encoder weights
are frozen and a linear model is trained on top to perform object-counting and depth prediction on the Clevr
dataset [43]. Compared to view-invariance methods such
as DINO and iBOT, the I-JEPA method effectively captures low-level image features during pretraining and outperforms them in object counting (Clevr/Count) and (by a
large margin) depth prediction (Clevr/Dist).

Low-Shot ImageNet-1K. Table 2 shows performance on
the 1% ImageNet benchmark. Here the idea is to adapt
the pretrained models for ImageNet classification using
only 1% of the available ImageNet labels, corresponding
to roughly 12 or 13 images per class. Models are adapted
via fine-tuning or linear-probing, depending on whichever
works best for each respective method. I-JEPA outperforms MAE while requiring less pretraining epochs when
using a similar encoder architecture. I-JEPA, using a ViTH/14 architecture, matches the performance of a ViT-L/16
pretrained with data2vec [8], while using significantly less
computational effort (see Section 7). By increasing the image input resolution, I-JEPA outperforms previous methods
including joint-embedding methods that do leverage extra
hand-crafted data-augmentations during pretraining, such
as MSN [4], DINO [17], and iBOT [79].
Transfer learning. Table 3 shows performance on various downstream image classification tasks using a linear
probe. I-JEPA significantly outperforms previous methods
that do not use augmentations (MAE and data2vec), and decreases the gap with the best view-invariance-based methods, which leverage hand-crafted data augmentations during pretraining, even surpassing the popular DINO [18] on
CIFAR100 and Place205 with a linear probe.

6. Local Prediction Tasks
As demonstrated in Section 5, I-JEPA learns semantic
image representations that significantly improve the downstream image classification performance of previous methods, such as MAE and data2vec. Additionally, I-JEPA benefits from scale and can close the gap, and even surpass,

7. Scalability
Model Efficiency. I-JEPA is highly scalable compared
to previous approaches. Figure 5 shows semi-supervised
evaluation on 1% ImageNet-1K as a function of GPU
hours. I-JEPA requires less compute than previous methods
and achieves strong performance without relying on handcrafted data-augmentations. Compared to reconstructionbased methods, such as MAE, which directly use pixels as
targets, I-JEPA introduces extra overhead by computing targets in representation space (about 7% slower time per iteration). However, since I-JEPA converges in roughly 5√ó
fewer iterations, we still see significant compute savings in
practice. Compared to view-invariance based methods, such
as iBOT, which rely on hand-crafted data augmentations to
create and process multiple views of each image, I-JEPA
also runs significantly faster. In particular, a huge I-JEPA
model (ViT-H/14) requires less compute than a small iBOT
model (ViT-S/16).
Scaling data size. We also find I-JEPA to benefit from
pretraining with larger datasets. Table 5 shows transfer

Pretrain

Arch.

IN1k
IN22k
IN22k

ViT-H/14
ViT-H/14
ViT-G/16

CIFAR100

Place205

INat18

Clevr/Count

Clevr/Dist

87.5
89.5
89.5

58.4
57.8
59.1

47.6
50.5
55.3

86.7
88.6
86.7

72.4
75.0
73.0

Table 5. Ablating dataset and model size. Evaluating impact of pre-training dataset size and model size on transfer tasks. I-JEPA
benefits from larger more diverse datasets. When increasing the size of the pretraining dataset (IN1k versus IN22k) we see an performance
improvement for the ViT-H/14 model. We observe a further performance improvement on semantic tasks by training a larger model ViTG/16 model on ImageNet-22k. The ViT-H/14 is trained for 300 epochs on IN1k and the equivalent of 900 IN1K epochs on IN22k. The
ViT-H/16 is trained for the equivalent of 600 IN1k epochs.

Semi-Supervised ImageNet-1K 1% Evaluation vs GPU Hours

8. Predictor Visualizations

80
ViT-H/16448

(300ep)

75

ViT-H/14

ViT-L/16

Top 1 (%)

(300ep)

(1600ep)

ViT-H/14

ViT-B/16

70

(1600ep)

(400ep)

ViT-L/16
(1600ep)

65
ViT-B/16

I-JEPA
iBOT
MAE

ViT-S/16

(600ep)

(800ep)

data2vec

60
103

104

Pretraining GPU Hours

Figure 5. Scaling. Semi-supervised evaluation on ImageNet-1K
1% as a function of pretraining GPU hours. I-JEPA requires less
compute than previous methods to achieve strong performance.
Compared to MAE and data2vec, I-JEPA obtains a significant
speedup by requiring fewer pretraining epochs. Compared to
iBOT, which relies on hand-crafted data-augmentations, a huge IJEPA model (ViT-H/14) requires less compute than their smallest
model (ViT-S/16).

The role of the predictor in I-JEPA is to take the output
of the context encoder and, conditioned on positional mask
tokens, to predict the representations of a target black at the
location specified by the mask tokens. One natural question
is whether the predictor conditioned on the positional mask
tokens is learning to correctly capture positional uncertainty
in the target. To qualitatively investigate this question, we
visualize the outputs of the predictor. We use the following
visualization approach to enable the research community to
independently reproduce our findings. After pretraining, we
freeze the context-encoder and predictor weights, and train
a decoder following the RCDM framework [13] to map the
average-pool of the predictor outputs back to pixel space.
Figure 6 shows decoder outputs for various random seeds.
Qualities that are common across samples represent information that is contained in the average-pooled predictor representation. The I-JEPA predictor correctly captures positional uncertainty and produces high-level object parts with
the correct pose (e.g., back of the bird and top of the car).

9. Ablations

learning performance on semantic and low level tasks when
increasing the size of the pretraining dataset (IN1K versus
IN22K). Transfer learning performance on these conceptually different tasks improves when pretraining on a larger
more diverse dataset.

Predicting in representation space. Table 7 compares
low-shot performance on 1% ImageNet-1K using a linear
probe when the loss is computed in pixel-space versus representation space. We conjecture that a crucial component
of I-JEPA is that the loss is computed entirely in representation space, thereby giving the target encoder the ability
to produce abstract prediction targets, for which irrelevant
pixel-level details are eliminated. From Table 7, it is clear
that predicting in pixel-space leads to a significant degradation in the linear probing performance.

Scaling model size. Table 5 also shows that I-JEPA benefit from larger model size when pretraining on IN22K.
Pretraining a ViT-G/16 significantly improves the downstream performances on image classification tasks such as
Place205 and INat18 compared to a ViT-H/14 model, but
does not improve performance on low-level downstream
tasks ‚Äî the ViT-G/16 uses larger input patches, which can
be detrimental for the local prediction tasks.

Masking strategy. Table 6 compare our multi-block
masking with other masking strategies such as
rasterized masking, where the image is split into
four large quadrants, and the goal is to use one quadrant
as a context to predict the other three quadrants, and the
traditional block and random masking typically used
in reconstruction-based methods. In block masking,
the target is a single image block and the context is the

Figure 6. Visualization of I-JEPA predictor representations. For each image: first column contains the original image; second column
contains the context image, which is processed by a pretrained I-JEPA ViT-H/14 encoder. Green bounding boxes in subsequent columns
contain samples from a generative model decoding the output of the pretrained I-JEPA predictor, which is conditioned on positional mask
tokens corresponding to the location of the green bounding box. Qualities that are common across samples represent information that
contained is in the I-JEPA prediction. The I-JEPA predictor is correctly capturing positional uncertainty and producing high-level object
parts with a correct pose (e.g., the back of the bird and top of a car). Qualities that vary across samples represent information that is not
contained in the representation. In this case, the I-JEPA predictor discards the precise low-level details as well as background information.
Targets

Context

Mask

Type

multi-block

Block(0.15, 0.2)
Quadrant
Block(0.6)
Random(0.6)

rasterized
block
random
‚àó Avg.

Freq.
4
3
1
1

Type
Block(0.85, 1.0) √ó Complement
Complement
Complement
Complement

Avg. Ratio‚àó

Top-1

0.25
0.25
0.4
0.4

54.2
15.5
20.2
17.6

Ratio is the average number of patches in the context block relative to the total number of patches in the image.

Table 6. Ablating masking strategy. Linear evaluation on ImageNet-1K using only 1% of the available labels after I-JEPA pretraining of
a ViT-B/16 for 300 epochs. Comparison of proposed multi-block masking strategy. In rasterized masking the image is split into
four large quadrants; one quadrant is used as a context to predict the other three quadrants. In block masking, the target is a single image
block and the context is the image complement. In random masking, the target is a set of random image patches and the context is the
image complement. The proposed multi-block masking strategy is helpful for guiding I-JEPA to learn semantic representations.

Targets

Arch.

Target-Encoder Output
Pixels

ViT-L/16
ViT-L/16

Epochs

Top-1

500
800

66.9
40.7

Table 7. Ablating targets. Linear evaluation on ImageNet-1K
using only 1% of the available labels. The semantic level of the
I-JEPA representations degrades significantly when the loss is applied in pixel space, rather than representation space, highlighting
the importance of the target-encoder during pretraining.

image complement. In random masking, the target is
a set of random patches and the context is the image
complement. Note that there is no overlap between the
context and target blocks in all considered strategies. We

find multi-block masking helpful for guiding I-JEPA
to learning semantic representations. Additional ablations
on multi-block masking can be found in Appendix C.

10. Conclusion
We proposed I-JEPA, a simple and efficient method for
learning semantic image representations without relying on
hand-crafted data augmentations. We show that by predicting in representation space, I-JEPA converges faster than
pixel reconstruction methods and learns representations of
a high semantic level. In contrast to view-invariance based
methods, I-JEPA highlights a path for learning general representations with joint-embedding architectures, without relying on hand-crafted view augmentations.

References
[1] Yuki Markus Asano, Christian Rupprecht, and Andrea
Vedaldi. Self-labelling via simultaneous clustering and representation learning. Internatinoal Conference on Learning
Representations, 2020. 1
[2] Mahmoud Assran, Randall Balestriero, Quentin Duval, Florian Bordes, Ishan Misra, Piotr Bojanowski, Pascal Vincent,
Michael Rabbat, and Nicolas Ballas. The hidden uniform
cluster prior in self-supervised learning. International Conference on Learning Representations, 2023. 1, 13
[3] Mahmoud Assran, Nicolas Ballas, Lluis Castrejon, and
Michael Rabbat. Supervision accelerates pre-training in contrastive semi-supervised learning of visual representations.
NeurIPS Workshop on Self-Supervised Learning, 2020. 13
[4] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes, Pascal Vincent, Armand Joulin,
Michael Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient learning. European Conference on
Computer Vision, 2022. 1, 2, 3, 5, 6, 12, 13, 16, 17
[5] Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand Joulin, Nicolas Ballas, and Michael Rabbat. Semi-supervised learning of visual features by nonparametrically predicting view assignments with support
samples. IEEE/CVF International Conference on Computer
Vision, 2021. 3, 13
[6] Philip Bachman, R Devon Hjelm, and William Buchwalter.
Learning representations by maximizing mutual information
across views. Advances in neural information processing
systems, 32, 2019. 13
[7] Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael
Auli. Efficient self-supervised learning with contextualized
target representations for vision, speech and language. arXiv
preprint arXiv:2212.07525, 2022. 5
[8] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework
for self-supervised learning in speech, vision and language.
arXiv preprint arXiv:2202.03555, 2022. 1, 3, 4, 5, 6, 13
[9] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training
of image transformers. arXiv preprint arXiv:2106.08254,
2021. 1, 3, 4, 13
[10] Adrien Bardes, Jean Ponce, and Yann LeCun.
Vicreg: Variance-invariance-covariance regularization for selfsupervised learning. arXiv preprint arXiv:2105.04906, 2021.
1, 3, 13
[11] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Selfsupervised learning of local visual features. arXiv preprint
arXiv:2210.01571, 2022. 1, 13
[12] Florian Bordes, Randall Balestriero, Quentin Garrido,
Adrien Bardes, and Pascal Vincent. Guillotine regularization: Improving deep networks generalization by removing
their head. arXiv preprint arXiv:2206.13378, 2022. 13
[13] Florian Bordes, Randall Balestriero, and Pascal Vincent.
High fidelity visualization of what your self-supervised representation knows about. Transactions on Machine Learning
Research, 2022. 7, 16
[14] John Bridle, Anthony Heading, and David MacKay. Unsupervised classifiers, mutual information and‚Äôphantom tar-

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

[26]

[27]

[28]

gets. Advances in neural information processing systems, 4,
1991. 13
Jane Bromley, James W Bentz, LeÃÅon Bottou, Isabelle Guyon,
Yann LeCun, Cliff Moore, Eduard SaÃàckinger, and Roopak
Shah. Signature verification using a ‚Äúsiamese‚Äù time delay
neural network. International Journal of Pattern Recognition
and Artificial Intelligence, 7(04):669‚Äì688, 1993. 1, 3
Zhaowei Cai, Avinash Ravichandran, Paolo Favaro,
Manchen Wang, Davide Modolo, Rahul Bhotika, Zhuowen
Tu, and Stefano Soatto. Semi-supervised vision transformers
at scale. arXiv preprint arXiv:2208.05688, 2022. 13
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning
of visual features by contrasting cluster assignments. arXiv
preprint arXiv:2006.09882, 2020. 1, 6
Mathilde Caron, Hugo Touvron, Ishan Misra, HerveÃÅ JeÃÅgou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv
preprint arXiv:2104.14294, 2021. 1, 3, 4, 5, 6, 12, 13
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International Conference on Machine Learning, pages 1691‚Äì1703. PMLR, 2020. 13
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning
of visual representations. preprint arXiv:2002.05709, 2020.
1, 2, 13
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad
Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. arXiv preprint
arXiv:2006.10029, 2020. 5
Xiaokang Chen, Mingyu Ding, Xiaodi Wang, Ying Xin,
Shentong Mo, Yunhao Wang, Shumin Han, Ping Luo,
Gang Zeng, and Jingdong Wang. Context autoencoder
for self-supervised representation learning. arXiv preprint
arXiv:2202.03026, 2022. 5
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
Improved baselines with momentum contrastive learning.
arXiv preprint arXiv:2003.04297, 2020. 12, 13
Xinlei Chen and Kaiming He. Exploring simple siamese
representation learning. arXiv preprint arXiv:2011.10566,
2020. 1, 3, 13
Xinlei Chen, Saining Xie, and Kaiming He. An empirical
study of training self-supervised vision transformers. arXiv
preprint arXiv:2104.02057, 2021. 4
Yubei Chen, Adrien Bardes, Zengyi Li, and Yann LeCun.
Intra-instance vicreg: Bag of self-supervised image patch
embedding. arXiv preprint arXiv:2206.08954, 2022. 13
Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detection. In 2005 IEEE computer society conference on computer vision and pattern recognition
(CVPR‚Äô05), volume 1, pages 886‚Äì893. Ieee, 2005. 4
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
Toutanova.
Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint
arXiv:1810.04805, 2018. 1

[29] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint
arXiv:2010.11929, 2020. 3, 4, 12, 13
[30] Alaaeldin El-Nouby, Gautier Izacard, Hugo Touvron, Ivan
Laptev, HerveÃÅ Jegou, and Edouard Grave. Are large-scale
datasets necessary for self-supervised pre-training? arXiv
preprint arXiv:2112.10740, 2021. 13
[31] Karl Friston. A theory of cortical responses. Philosophical transactions of the Royal Society B: Biological sciences,
360(1456):815‚Äì836, 2005. 1
[32] Spyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick
PeÃÅrez, and Matthieu Cord. Learning representations by
predicting bags of visual words. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 6928‚Äì6938, 2020. 13
[33] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep
learning. MIT press, 2016. 13
[34] Priya Goyal, Quentin Duval, Jeremy Reizenstein, Matthew
Leavitt, Min Xu, Benjamin Lefaudeux, Mannat Singh,
Vinicius Reis, Mathilde Caron, Piotr Bojanowski, Armand
Joulin, and Ishan Misra. Vissl. https://github.com/
facebookresearch/vissl, 2021. 12
[35] Jean-Bastien Grill, Florian Strub, Florent AltcheÃÅ, Corentin
Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A
new approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020. 1, 3, 5, 12, 13
[36] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
DollaÃÅr, and Ross Girshick. Masked autoencoders are scalable
vision learners. IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2022. 1, 2, 3, 4, 5, 6, 12, 13, 15, 16
[37] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross
Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.
1, 3, 12, 13
[38] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770‚Äì778, 2016. 3
[39] Olivier Henaff. Data-efficient image recognition with contrastive predictive coding. In International conference on
machine learning, pages 4182‚Äì4192. PMLR, 2020. 13
[40] R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon,
Karan Grewal, Phil Bachman, Adam Trischler, and Yoshua
Bengio.
Learning deep representations by mutual information estimation and maximization. arXiv preprint
arXiv:1808.06670, 2018. 13
[41] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto,
and Masashi Sugiyama. Learning discrete representations
via information maximizing self-augmented training. In International conference on machine learning, pages 1558‚Äì
1567. PMLR, 2017. 13
[42] Justin Johnson, Bharath Hariharan, Laurens Van
Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross

[43]

[44]

[45]
[46]

[47]

[48]
[49]

[50]
[51]
[52]

[53]

[54]

[55]

[56]

[57]

[58]

Girshick. Clevr: A diagnostic dataset for compositional
language and elementary visual reasoning. In Proceedings
of the IEEE conference on computer vision and pattern
recognition, pages 2901‚Äì2910, 2017. 12
Justin Johnson, Bharath Hariharan, Laurens van der Maaten,
Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr:
A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, 2017. 6
Andreas Krause, Pietro Perona, and Ryan Gomes. Discriminative clustering by regularized information maximization. Advances in neural information processing systems, 23,
2010. 13
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple
layers of features from tiny images. 2009. 12
Gustav Larsson,
Michael Maire,
and Gregory
Shakhnarovich. Learning representations for automatic
colorization. 2016. 4
Gustav Larsson,
Michael Maire,
and Gregory
Shakhnarovich. Colorization as a proxy task for visual
understanding. 2017. 4
Yann LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. 2022. 2, 3
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and
Fujie Huang. A tutorial on energy-based learning. Predicting
structured data, 1(0), 2006. 2
Ralph Linsker. Self-organization in a perceptual network.
Computer, 21(3):105‚Äì117, 1988. 13
Ilya Loshchilov and Frank Hutter. Decoupled weight decay
regularization. arXiv preprint arXiv:1711.05101, 2017. 12
Yi Ma, Doris Tsao, and Heung-Yeung Shum. On the principles of parsimony and self-consistency for the emergence
of intelligence. Frontiers of Information Technology & Electronic Engineering, pages 1‚Äì26, 2022. 13
Ishan Misra and Laurens van der Maaten. Self-supervised
learning of pretext-invariant representations. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 6707‚Äì6717, 2020. 13
Jovana Mitrovic, Brian McWilliams, Jacob Walker, Lars
Buesing, and Charles Blundell. Representation learning via
invariant causal mechanisms. International Conference on
Learning Representations, 2021. 13
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv
preprint arXiv:1807.03748, 2018. 13
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.
12
Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor
Darrell, and Alexei A Efros. Context encoders: Feature
learning by inpainting. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
2536‚Äì2544, 2016. 1, 4
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.

[59]

[60]

[61]

[62]

[63]

[64]

[65]

[66]

[67]

[68]

[69]

[70]

Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821‚Äì8831. PMLR, 2021.
4
Rajesh PN Rao and Dana H Ballard. Predictive coding
in the visual cortex: a functional interpretation of some
extra-classical receptive-field effects. Nature neuroscience,
2(1):79‚Äì87, 1999. 1
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. Imagenet large scale visual recognition challenge.
International Journal of Computer Vision, 115(3):211‚Äì252,
2015. 5, 12
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. arXiv preprint
arXiv:1703.01780, 2017. 12
Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In International Conference on Machine
Learning, pages 10268‚Äì10278. PMLR, 2021. 13
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and HerveÃÅ JeÃÅgou. Training
data-efficient image transformers & distillation through attention. In International Conference on Machine Learning,
pages 10347‚Äì10357. PMLR, 2021. 3
Michael Tschannen, Josip Djolonga, Paul K Rubenstein,
Sylvain Gelly, and Mario Lucic. On mutual information
maximization for representation learning. arXiv preprint
arXiv:1907.13625, 2019. 13
Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,
Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and
Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 8769‚Äì8778,
2018. 12
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In Advances in neural
information processing systems, pages 5998‚Äì6008, 2017. 3
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua
Bengio, Pierre-Antoine Manzagol, and LeÃÅon Bottou.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of machine learning research, 11(12), 2010. 1, 4, 13
Chen Wei, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan
Yuille, and Christoph Feichtenhofer. Masked feature prediction for self-supervised visual pre-training. arXiv preprint
arXiv:2112.09133, 2021. 1, 13
Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 3733‚Äì3742,
2018. 13
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong,
and Quoc V Le. Unsupervised data augmentation. arXiv
preprint arXiv:1904.12848, 2019. 13

[71] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin
Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. arXiv preprint
arXiv:2111.09886, 2021. 1, 4
[72] Yang You, Igor Gitman, and Boris Ginsburg. Large batch
training of convolutional networks, 2017. 12
[73] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk
Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6023‚Äì6032, 2019. 16
[74] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and
SteÃÅphane Deny. Barlow twins: Self-supervised learning via
redundancy reduction. arXiv preprint arXiv:2103.03230,
2021. 1, 3, 13
[75] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov,
Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen,
Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil
Houlsby. A large-scale study of representation learning with
the visual task adaptation benchmark, 2019. 12
[76] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and
David Lopez-Paz. mixup: Beyond empirical risk minimization. Internatinoal Conference on Learning Representations,
2018. 16
[77] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful
image colorization. 2016. 4
[78] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene
recognition using places database. Advances in neural information processing systems, 27, 2014. 12
[79] Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang
Xie, Alan Yuille, and Tao Kong. Ibot: Image bert pretraining with online tokenizer. International Conference on
Learning Representations, 2022. 2, 4, 5, 6, 12, 13

A. Implementation Details
A.1. Pretraining
Architectures. For I-JEPA pretraining, we use Vision Transformer [29] (ViT) architectures for the context-encoder, targetencoder, and the predictor. While the context-encoders and target-encoders correspond to standard ViT architectures, the
predictor is designed as a light-weight (narrow) ViT architecture. Specifically, we fix the embedding dimension of the
predictor to 384, while keeping the number of self-attention heads equal to that of the backbone context-encoder. For the
smaller ViT-B/16 context-encoder, we set the depth of the predictor to 6. For ViT-L/16, ViT-H/16, and ViT-H/14 contextencoders, we set the depth of the predictor to 12. Finally, the ViT-G/16 uses a predictor of depth 16. I-JEPA is pretrained
without a [cls] token. We use the target-encoder for evaluation and average pool its output to produce a global image
representation.

Optimization. We use AdamW [51] to optimize the context-encoder and predictor weights. Our default batch-size is 2048,
and the learning rate is linearly increased from 10‚àí4 to 10‚àí3 during the first 15 epochs of pretraining, and decayed to 10‚àí6
following a cosine schedule thereafter. Following [4, 18], the weight-decay is linearly increased from 0.04 to 0.4 throughout
pretraining. The target-encoder weights are identical to the context-encoder weights at initialization, and updated via an
exponential moving average thereafter [4, 18, 23, 35, 37, 61]. We use a momentum value of 0.996, and linearly increase this
value to 1.0 throughout pretraining, following [4, 18].
Masking. By default, we sample 4 possibly overlapping target blocks masks with random scale in the range (015, 0.2) and
aspect ratio in the range (0.75, 1.5). We sample 1 context block mask with random scale in the range (0.85, 1.0) and unit
aspect ratio. We subsequently eliminate any regions in the context block mask that overlap with any of the 4 target block
masks. The context-block mask and target-block masks are sampled independently for each image in the mini-batch. To
ensure efficient batch processing, we restrict the size of all context masks on a co-located GPU to be identical. Similarly, we
restrict the size of all target masks on a co-located GPU to be identical. The mask-sampler is efficiently implemented in only
a few lines of code in PyTorch [56] using a batch-collator function, which runs in the data loader processes. In short, in each
iteration, the data loader returns a mini-batch of images and a set of context and target masks for each image, identifying the
patch indices to keep for the context and target views.

A.2. Downstream Tasks
Linear evaluation. When evaluating methods such as iBOT [79], DINO [18] or MAE [36], which leverage Vision Transformers [29] with an additional [cls] token, we use the default configurations of VISSL [34] to evaluate all the models
on iNaturalist18 [65], CIFAR100 [45], Clevr/Count [42, 75], Clevr/Dist [42, 75], and Places205 [78]. We freeze the encoder
and return the best number among the following representations: 1) the [cls] token representation of the last layer, 2) the
concatenation of the last 4 layers of the [cls] token. For each representation, we try two different heads: 1) a linear head,
or 2) a linear head preceded by a batch normalization, and return the best number. We use the default data augmentations
of VISSL [34]: random resize cropping and horizontal flipping, with the exception of Clevr/Count and Clevr/Dist, where
we only use center crop and horizontal flipping, as random cropping interferes with the capability of counting objects and
estimating distance, removing critical objects from the scene. For CIFAR100, we resize the images to 224 √ó 224 pixels, so
as to keep the number of patches equal to that used during pretraining.
Because our I-JEPA implementation uses Vision Transformer architectures without a [cls] token, we adapt the default
VISSL evaluation recipe to utilize the average-pooled patch representation instead of the [cls] token. We therefore report
the best linear evaluation number among the following representations: 1) the average-pooled patch representation of the
last layer, 2) the concatenation of the last 4 layers of the average-pooled patch representations. We otherwise keep the
linear-probing recipe identical.

ImageNet evaluations. To evaluate the I-JEPA on ImageNet [60], we adapt the VISSL recipe to use average pooled representations instead of the [cls] token. Following MAE [36], we use the LARS [72] optimizer with a batch-size of 16384,
and train the linear probe for 50 epochs. We use a learning rate with a step-wise decay, dividing it by a factor of 10 every 15
epochs, and sweep three different reference learning rates [0.01, 0.05, 0.001], and two weight decay values [0.0005, 0.0].

Low-shot evaluation. To evaluate our model on the ImageNet-1% low-shot task, we adapt the fine-tuning protocol of
MAE [36].We fine-tune our ViT-L/H models for 50 epochs on ImageNet-1% with the AdamW optimizer and a cosine learning
rate scheduler. We use a batch size of 512, a learning rate layer decay of 0.75 and 0.1 label smoothing. We use the default
randaugment data-augmentations as in MAE. In contrast to the fine-tuning done with MAE, we do not use mixup, cutmix,
random erasing or drop path. For the I-JEPA, we use a learning rate /weight decay of 3e‚àí5 /5e‚àí2 for the ViT-L/16, 3e‚àí5 /4e‚àí1
for the ViT-H/14 and 3e‚àí5 /4e‚àí1 for the ViT-H/16448 . Similar fine-tuning strategy for low-shot learning has been explored by
Semi-VIT in the context of semi-supervised learning [16].

B. Broader Related Work
Self-supervised learning of visual representations with joint-embedding architectures is an active line of research [3,
10, 12, 18, 23, 24, 35, 37, 54, 69, 79]. These approaches train a pair of encoders to output similar embeddings for two or
more views of the same image. To avoid pathological solution, many popular joint-embedding approaches use explicit
regularization [5, 10, 18, 20] or architectural constraints [24, 35]. Collapse-prevention based on architectural constraints
leverage specific network design choices to avoid collapse, for example, by stopping the gradient flow in one of the jointembedding branches [20], using a momentum encoder in one of the joint-embedding branches [35], or using an asymmetric
prediction head [8, 20, 35]. Recent work [62] attempts to theoretically understand (in certain simplified settings) how jointembedding methods with architectural constraints avoid representation collapse without explicit regularization.
Typical regularization-based approaches to collapse prevention in joint-embedding architectures try to maximize the volume of space occupied by the representations. This is often motivated through the InfoMax [52] principle. Indeed, a longstanding conviction in unsupervised representation learning is that the resulting representations should be both maximally
informative about the inputs, while also satisfying certain simplicity constraints [33,50]. The former objective is often referred
to as the information-maximization principle (InfoMax), while the latter is sometimes referred to as the parsimony principle [52]. Such approaches to representation learning have been proposed for decades (e.g., [14]), where, historically, simplicity constraints were enforced by encouraging the learned representations to be sparse, low-dimensional, or disentangled, i.e.,
the individual dimensions of the representation vector should be statistically independent [33]. Modern approaches enforce
the simplicity constraints coupled with InfoMax regularization through self-supervised loss terms [6, 40, 41, 44, 55, 64]. One
example is the widespread view-invariance penalty [53], often coupled with with independence [10,74] or low-dimensionality
constraints, e.g., by projecting representations on the unit hypersphere [20, 35, 37]. However, despite its proliferation, there
have also been many criticisms of the InfoMax principle, especially since it is does not discriminate between different types
of information (e.g, noise and semantics) [2]. Indeed, the sets of features we wish the model to capture are not always those
with the highest marginal entropy (maximal information content).
Orthogonal to the contributions of invariance-based pretraining, another line of work attempts to learn representations by
artificially masking parts of the input and training a network to reconstruct the hidden content [67]. Autoregressive models,
and denoising autoencoders in particular, predict clean visual inputs from noisy views [8, 9, 19, 36, 67]. Typically, the goal
is to predict missing inputs at a pixel level [29, 36, 70], or at a patch token-level, using a tokenizer [9, 68]. While these
works demonstrate impressive scalability, they usually learn features at a low-level of semantic abstraction compared to
joint-embedding approaches [4].
More recently, a set of approaches attempt to combine both joint-embedding architectures and reconstruction based approaches [30], wherein they combine an invariance pretraining loss with a patch-level reconstruction loss, as in the iBOT
method [79]. Since view-invariance based approaches are typically biased towards learning global image representations,
thereby limiting their applicability to other computer vision tasks, the idea is that adding local loss terms can improve performance on other popular tasks in computer vision [11, 26, 32]. The framework of contrastive predictive coding [55] is also
closely related to this line of work on local loss terms. In the context of images [39], here the idea is to use a contrastive
objective combined with a convolutional network to discriminate between overlapping image patch representations. Specifically, the goal is to encourage the representations of an image patch to be predictable of the image patches directly below
it, while pushing away the representations of other patch views. In contrast to that work, the proposed I-JEPA method is
non-contrastive and does not seek to discriminate between image patches. Rather, the goal is to predict the representations
of various target blocks from a single context block. This is achieved with a Joint-Embedding Predictive Architecture, using
a predictor network that is conditioned on positional embeddings corresponding to the location of the target block in the
image. Qualitative experiments in Section 8 show that the predictor network in our architecture learns to correctly perform
this local-to-local region feature mapping, and learns to correctly capture positional uncertainty in the image.

Targets

Context

Scale

Freq.

Scale

Top-1

(0.075, 0.2)
(0.1, 0.2)
(0.125, 0.2)
(0.15, 0.2)
(0.2, 0.25)
(0.2, 0.3)

4
4
4
4
4
4

(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)

19.2
39.2
42.4
54.2
38.9
33.6

Table 8. Ablation of the target block size for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the
available labels); ablating the multi-block target size during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Predicting larger (semantic)
blocks improves the low-shot accuracy as long as the context is sufficiently informative.
Targets

Context

Scale

Freq.

Scale

Top-1

(0.15, 0.2)
(0.15, 0.2)
(0.15, 0.2)
(0.15, 0.2)

4
4
4
4

(0.40, 1.0)
(0.65, 1.0)
(0.75, 1.0)
(0.85, 1.0)

31.2
47.1
49.3
54.2

Table 9. Ablation of the context size for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the available
labels); ablating the multi-block target size during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Reducing the multi-block context size
degrades the low-shot performance.
Targets

Context

Scale

Freq.

Scale

Top-1

(0.15, 0.2)
(0.15, 0.2)
(0.15, 0.2)
(0.15, 0.2)

1
2
3
4

(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)
(0.85, 1.0)

9.0
22.0
48.5
54.2

Table 10. Ablation of the targets number for multi-block masking. Linear evaluation on 1% ImageNet-1K (using only 1% of the
available labels); ablating the multi-block number of targets during I-JEPA pretraining of a ViT-B/16 for 300 epochs. Increasing the
number of target blocks improve the low-shot accuracy.

C. Additional Ablations
This section follows the same experimental protocol as Section 9. We report the result of a linear probe with a frozen
backbone, trained on the low-shot 1% ImageNet-1K benchmark.
Multiblock masking strategy. We present an extended ablation of the multiblock masking strategy where we change the
targets block scale (Table 8), the context scale (Table 9) and the number of target blocks (Table 10). We train a ViT-B/16
for 300 epochs using I-JEPA with various multi-block settings and compare performance on the 1% ImageNet-1K
benchmark using a linear probe. In short, we find that it is important to predict several relatively large (semantic) target
blocks, and to use a sufficiently informative (spatially distributed) context block.
Masking at the output of the target-encoder. An important important design choice in I-JEPA is that the target blocks
are obtained by masking the output of the target-encoder, not the input. Table 11 shows the effect of this design choice
on the semantic level of the learned representations when pretraining a ViT-H/16 using I-JEPA for 300 epochs. In the case
where masking is applied to the input, we forward-propagate through the target-encoder once for each target region. Masking
the output of the target-encoder during pretraining results in more semantic prediction targets and improves linear probing
performance.

Target Masking

Arch.

Output
Input

ViT-H/16
ViT-H/16

Epochs

Top-1

300
300

67.3
56.1

Table 11. Ablating masking output of target encoder. Linear evaluation on ImageNet-1K using only 1% of the available labels; ablating
the effect of masking the target-encoder output during I-JEPA pretraining of a ViT-H/16 for 300 epochs. Masking the output of the
target-encoder during pretraining significantly improves the linear probing performance of the pretrained representations.

Predictor depth. We examine the impact of the predictor depth on the downstream low-shot performance in Table 12.
We pretrain a ViT-L/16 for 500 epochs using either a 6-layer predictor network or a 12-layer predictor network. The model
pretrained using a deeper predictor shows a significant improvement in downstream low-shot performance compared to the
model pretrained with a shallower predictor.
Predictor Depth

Arch.

6
12

ViT-L/16
ViT-L/16

Epochs

Top-1

500
500

64.0
66.9

Table 12. Ablating the predictor depth. Linear evaluation on ImageNet-1K using only 1% of the available labels; ablating the effect of
masking the predictor depth for a ViT-L/16 pretrained for 500 epochs. Increasing the predictor depth leads to significant improvement of
the linear probe performance of the pretrained representations.

Weight decay. In Table 13, we evaluate the impact of weight-decay during pretraining. We explore two weight decay
strategies: linearly increase the weight-decay from 0.04 to 0.4 or use a fix weight-decay of 0.05. Using a smaller weight
decay during pretraining improves the downstream performance on ImageNet-1% when fine-tuning. However, this also leads
to a degradation of performance in linear evaluation. In the main paper, we use the first weight decay strategy as it improves
the performances in linear evaluation downstream tasks.
Weight Decay

Arch.

0.04 ‚Üí 0.4
0.05

ViT-L/16
ViT-L/16

Epochs

ImageNet-1%

ImageNet Linear-Eval

600
600

69.4
70.7

77.8
76.4

Table 13. Ablating the pretraining weight-decay. We compare our default pretraining weight decay strategy where we linearly increase
the weight-decay from 0.04 to 0.4 to using a fix weight decay of 0.05. Using a smaller weight-decay during pretraining can improve the
fine-tuning performance on ImageNet-1%, However, it also leads to a drop of performance in linear evaluation.

Predictor width. We explore the impact of the predictor width in Table 14. We compare I-JEPA using a ViT-L encoder
and a predictor with 386 channels to a similar model using a predictor with 1024 channels. Note that the ViT-L encoder has
1024 channels. Using a bottleneck in the predictor width improves the downstream performance on ImageNet 1%.
Predictor Width

Arch.

384
1024

ViT-L/16
ViT-L/16

Epochs

Top-1

600
600

70.7
68.4

Table 14. Ablating the predictor width. We reports results on ImageNet-1K 1% using fine-tuning. We compare two predictors having a
width of either 384 or 1024. Note the I-JEPA encoder is a ViT-L with 1024 channels. Having a width bottleneck in the predictor improves
the downstream performances.

D. Finetuning on the full ImageNet
In this section, we report performance on I-JEPA when fine-tuning on the full ImageNet dataset. We focus on the ViTH/16448 as this architecture achieves state-of-art performance with MAE [36].

We use a fine-tuning protocol similar to MAE. Specifically, we fine-tune our model for 50 epochs using AdamW and a
cosine learning rate schedule. The base learning rate is set to 10‚àí4 and the batch size to 528. We train using mixup [76] set
to 0.8, cutmix [73] set to 1.0, a drop path probability of 0.25 and a weight decay set to 0.04. We also use a layer decay of
0.75. Finally, we use the same rand-augment data-augmentations as MAE,
Table 15 reports the fine-tuning results. I-JEPA achieves 87.1 top-1 accuracy. Its performance is less than 1% away from
the best MAE model despite I-JEPA being trained for 5.3 times less epochs than MAE. This result demonstrates that I-JEPA
is competitive when fine-tuning on the full ImageNet dataset.

Method

Arch.

MAE [36]
I-JEPA

ViT-H/14448
ViT-H/16448

Epochs

Top-1

1600
300

87.8
87.1

Table 15. Finetuning on the full ImageNet dataset. I-JEPA achieves competitive performance. I-JEPA is close to MAE approach despite
I-JEPA being trained for 5.3 times less epochs than MAE.

E. RCDM Visualizations
To visualize the representations of a pretrained neural network in pixel space, we use the RCDM framework [13]. The
RCDM framework trains a decoder network hœâ , comprising a generative diffusion model, to reconstruct an image x from
the representation vector of that image sx and a noisy version of that image xÃÇ := x + œµ, where œµ is an additive noise vector.
Concretely, the decoder objective is to minimize the loss function ‚à•hœâ (xÃÇ, sx )‚àíœµ‚à•. We train each RCDM network for 300,000
iterations using the default hyperparameters [13]. After training the decoder, one can subsequently feed the representation
vector of an unseen test image sy into the decoder along with various random noise vectors to generate several pixel-level
visualizations of the representation, thus providing insight into the features captured in the representations of the pretrained
network. Qualities that are common across samples represent information that is contained in the representation. On the
other hand, qualities that vary across samples represent information that is not contained in the representations
In Figure 6, the visualizations are obtained by feeding the average-pooled output of the predictor, conditioned on a specific
target region, into the decoder network, along with various random noise vectors. In Figures 7 and 8, the visualizations are
obtained by feeding the average-pooled output of the target-encoder into the decoder network, along with various random
noise vectors.

E.1. Encoder Visualization
In Figure 7, we visualize the average-pooled I-JEPA representations at the output of our ViT-H/14 target-encoder. The first
column contains the original image, while subsequent columns contain synthetic samples obtained by feeding the averagepooled representation of the image into the decoder along with various random noise vectors. Figure 7 suggests that the IJEPA target-encoder is able to correctly capture the high-level information regarding objects and their poses, while discarding
low-level image details and background information.
Figure 8 shows similar visualizations, but when using an MSN [4] pretrained ViT-L/7 target-encoder to compute the
image representations. The MSN method trains a context- and target-encoder using a Joint-Embedding Architecture to
enforce invariance of global image representations to various hand crafted data augmentations and missing patches. While
the MSN pretrained network is able to capture high level semantic information about the image in the first column, it also
exhibits higher variability in the generated samples, e.g., variability in the object pose, object scale, and number of instances.
In short, the MSN pretrained discards much of the local structure in the image, which is in stark contrast to I-JEPA, which
retains information about much of the local structure in the input image.

Figure 7. Visualization of I-JEPA target-encoder representations. For each image: first column contains the original image; subsequent
columns contain samples from a generative model decoding the average-pooled output of a pretrained I-JEPA target-encoder. Qualities
that are common across samples represent information that contained is in the I-JEPA representation. I-JEPA is able to correctly capture
the high-level information regarding objects and their poses. Qualities that vary across samples represent information that is not contained
in the representation. I-JEPA encoder discards the precise low-level details as well as background information.

Figure 8. Visualization of MSN target-encoder representations. For each image: first column contains the original image; subsequent
columns contain samples from a generative model decoding the output of a frozen MSN encoder [4]. Qualities that are common across
samples represent information that is contained in the representation. Qualities that vary across samples represent information that is not
captured by MSN. Compared to I-JEPA, MSN samples show higher variability. MSN retains less information from the input. In particular,
it discards global structure information such as the object pose or even number of instances.

